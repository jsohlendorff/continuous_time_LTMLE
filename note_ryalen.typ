#import "@preview/fletcher:0.5.1": node, edge, diagram
// #import "template.typ": conf
#import "template/definitions.typ": *
#import "@preview/arkheion:0.1.0": arkheion
#import "@preview/ctheorems:1.1.3": *
#let definition = thmbox("definition", "Definition", inset: (x: 1.2em, top: 1em))
#let theorem = thmbox("theorem", "Theorem", fill: rgb("#eeffee"))
#import "@preview/numty:0.0.5" as nt
#set cite(form: "prose")
// Color references red
#show  ref: it => {text(fill: maroon)[#it]}

#let theorem = thmbox("theorem", "Theorem", fill: rgb("#eeffee"))
#let proof = thmproof("proof", "Proof")

#set math.equation(numbering: "(1)")
#show math.equation: it => {
  if it.block and not it.has("label") [
    #counter(math.equation).update(v => v - 1)
    #math.equation(it.body, block: true, numbering: none)#label("")
  ] else {
    it
  }  
}

#show: thmrules.with(qed-symbol: $square$)

// In discrete time, the two formula are the same because every discrete time is a visitation event?
// What if we are still in discrete time, but the treatment decision is not made at every time point, but randomly; then it maybe does not relate to discrete time theories
// It appears then that the formulas can still be different. 

= A counterfactual interpretation of target parameter in @rytgaardContinuoustimeTargetedMinimum2022

Let us consider a setting similar to the one of @ryalenPotentialOutcomes.
It has been discussed in discrete time (@RobinsLongitudinal2001)
that the g-formula is unique; however, as we shall
see the g-formula in continuous time may not necessarily be uniquely defined.
Specifically this may relate to conditional distributions
in this setting not being uniquely defined.
We will work with an intervention
that specifies the treatment decisions
but not the timing of treatment visits.
We consider death as the outcome of interest
and are interested in the probability of death,
had had we followed the regime of always treating.
To simplify,
we work without right-censoring, no covariates,
and compliance to treatment at time $0$.
Let $(Omega, cal(F), P)$ be a probability space.
and consider $(N^y, N^a)$,
where
- $N^y$ is a counting process on $[0, T]$ for death.
- $N^a$ is a random measure for treatment on $[0, T] times {1, 0}$, where $1$ denotes treatment and $0$ no treatment.

We consider the filtration generated by $(N^y, N^a)$ and denote it by $(cal(F)_t)_(t >= 0)$,
i.e.,
$
    cal(F)_t := sigma(N^y (dif s), N^a (dif s times {x}) | s in (0, t], x in {0, 1}).
$
Further, we assume that
- $N^y$ and $N^a ({(0, t] times {1, 0}})$ do not jump at the same time.
- $M^y = N^y - Lambda^y$ denotes their $P$-$cal(F)_t$ (local) martingale,
  where $Lambda^y$ is the $P$-$cal(F)_t$-compensator of $N^y$.
- $M^a (dif t times {x}) = N^a (dif t times {x}) - (pi_t)^(bb(1) {x=1}) (1-pi_t)^(bb(1) {x=0}) Lambda^a (dif t)$
  is the $P$-$cal(F)_t$ (local) martingale for $x in {1, 0}$,
  where $pi_t$ is the $cal(F)_t$-predictable probability of treatment at time $t$ (mark probability) and
  $Lambda^a (dif t)$ is the total $P$-$cal(F)_t$-compensator of $N^a (dif t times dif x)$.

For this treatment regime, we see that 
$
    tau^A = inf {t >= 0 | N^a ((0, t] times {0}) > 0}.
$
// We can associate each of the random measures $N^y$ and $N^a$ with
// the random measure
// $
//     N (dif (t, m, a)) = N^y (dif t) delta_y (dif m) + delta_a (dif m) {N^a (dif (t) times {1}) delta_(1) (dif m) + N^a (dif (t) times {0}) delta_(0) (dif m)}.
// $
// We can then find that $N$ has the $P$-$cal(F)_t$-compensator,
// $
//     Lambda (dif (t, m, a)) = Lambda^y (dif t) delta_y (dif m) + delta_a (dif m) {pi_t (cal(F)_(t-)) delta_(1) (dif m) +  (1-pi_t (cal(F)_(t-))) delta_(0) (dif m)} Lambda^a (dif (t)),
// $
//where we can choose $pi_t$ to be $cal(F)_t$-predictable.
We are interested in the counterfactual mean outcome $mean(P) [tilde(Y)_t]$,
where $(tilde(Y)_t)_(t >= 0)$ is the counterfactual outcome process
of $Y := N^y$ under the intervention that sets treatment to $1$ at all visitation times.
This process is assumed to satisfy the definition of counterfactual outcome processes
of @ryalenPotentialOutcomes with their Example 4. 
Note the different exchangeability condition compared to @ryalenPotentialOutcomes,
as @ryalenPotentialOutcomes expresses exchangeability through the counting process $bb(1) {tau^A <= dot}$.
//this appears to me to be a weaker condition (?).
Let $(event(k), status(k), treat(k))$
denote the ordered event times, event types, and treatment decisions at event $k$.
Note that @eq:rytgaard is the same likelihood ratio as in @rytgaardContinuoustimeTargetedMinimum2022.
We also impose the assumption that $N_t := N^y_t + N^a ( {(0, t] times {1, 0}})$ does not explode;
we also assume that we work with a version of the compensator
such that $Lambda ({t} times {y, a} times {1, 0}) < oo$ for all $t > 0$.
We may generally also work with a compensator $Lambda$ that fulfills conditions (10.1.11)-(10.1.13) of @last1995marked.
Let $pi_(event(k))^* (history(k-1))$ denote the interventional probability,
which in this case we take to be $1$.
In this case,
$
    pi_t &= sum_k bb(1) {event(k-1) < t < event(k)} pi_(event(k)) (history(k-1)) \
    pi^*_t &= sum_k bb(1) {event(k-1) < t < event(k)} pi^*_(event(k)) (history(k-1)) = 1.
$
Let $N^(a x) (dif t) := N^a (dif t times {x})$ for $x in {1, 0}$.

*NOTES:*
- Does the exchangeability condition simplify in the case of $cal(n)^a$ predictable in $P$-$cal(F)_t$
  as specified in @ryalenPotentialOutcomes; as noted in their article the two likelihood ratios
  turn out two be the same in the case of orthogonal martingales.

  Suppose that $cal(n)^a$ is predictable
  so that $N^(a 1) (dif t)$ is predictable
  in that case the first exchangeability condition is trivial;
  Pål's condition only grants exchangeability for
  $N^(a 1) (t and tau^A) $ is predictable;
  I think that this is the sufficient for the argument to go through. 

- Positivity holds for example if $pi_t$ is bounded away from $0$ and $1$
  and $N_t$ has bounded number of jumps in the study period. 

// UI (Uniform integrability) implies zeta (t) = integral ... is uniformly integrable martingale,
// but why is integral_0^t tilde(Y)_(t) zeta (dif s) a martingale (generally a martingale)

// If there are two solutions (mine and theirs), they may be an infinite number of solutions
// due to convexity
//
// Definition of Potential outcome process
//

// Necessary criteria for the conclusion to hold
// for two measures Q in terms of P
// 1. E_P [tilde(Y)_t] = E_Q [Y_t]
// 2. Q(tau^A > t) = 1
// 3. Q and P are equivalent on cal(F)_t and the likelihood ratio is W(t)

// Look for restrictions on such that positivity in one situation implies the other
// It is natural to ask what the differences between the EIF in mine and Pål's paper are
// and whether the variance of one is smaller than the other?

// E_P[tilde(W)_t Y_t] = E_P[W_t Y_t]; can this hold? yes if E_P[tilde(W)_t | Y_t, W_t] = 1?
// Pål assumes that exchangeability holds for the weights; therefore E[tilde(Y)_t W_t] = E[tilde(Y)_t W_0]
// by conditioning.
// CAR is stronger than exchangeability => likelihood factorizes, but we do not need it to. 
#theorem[
If _all_ of the following conditions hold:
- *Consistency*: $tilde(Y)_(dot) bb(1) {tau^a > dot} = Y_(dot) bb(1) {tau^a > dot} quad P-"a.s."$
- *Exchangeability*:
  Define $cal(H)_t := cal(F)_t or sigma(tilde(Y))$.
  The $P$-$cal(F)_t$ compensator for $N^a$ is also the $P$-$cal(H)_t$ compensator.
- *Positivity*: 
  $
      W (t) := product_(j = 1)^(N_t) (((pi^*_(event(j)) (history(j-1))) / (pi_(event(j)) (history(j-1))))^(bb(1) {treat(k) = 1}) ((1-pi^*_(event(j)) (history(j-1))) / (1-pi_(event(j)) (history(j-1))))^(bb(1) {treat(k) = 0}))^(bb(1) {status(j) = a}) 
  $ <eq:rytgaard>
  is uniformly integrable.
  
  Furthermore, assume that $K(s) = ((pi_s^*)/(pi_s) - 1) M^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) M^(a 0) (dif s)$
  is a $P$-$cal(F)_t$-martingale and that $K$ is a process of *locally integrable variation*, meaning that $mean(P) [integral_0^t |d K(s)| ] < oo$ for all $t > 0$.

Then,
$
    mean(P) [tilde(Y)_t] = mean(P) [Y_t W (t)]
$
    and $W (t) = cal(E) (K)_t$ is a uniformly integrable $P$-$cal(F)_t$-martingale, where $cal(E)$ denotes the Doléans-Dade exponential (@protter2005stochastic).
] <thm:identifiabilitymartingale>

#proof[
    We shall use that the likelihood ratio solves a specific stochastic differential equation.
    To this end, note that
    $
        &W(t) \
            &= product_(s <= t) (1 + ((pi_s^*)/(pi_s) - 1) N^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) N^(a 0) (dif s)) \
            &= product_(s <= t) (1 + ((pi_s^*)/(pi_s) - 1) N^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) N^(a 0) (dif s) - (pi_s^* - pi_s) Lambda^(a) (d s) - (pi_s - pi_s^*) Lambda^(a) (d s)) \
            &= product_(s <= t) (1 + ((pi_s^*)/(pi_s) - 1) N^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) N^(a 0) (dif s) - (pi_s^* - pi_s) / pi_s Lambda^(a 1) (d s) - (pi_s - pi_s^*) / (1-pi_s) Lambda^(a 0) (d s)) \
            &= product_(s <= t) (1 + ((pi_s^*)/(pi_s) - 1) M^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) M^(a 0) (dif s)).        
    $
        
    Thus, by properties of the product integral (e.g., Theorem II.6.1 of @andersenStatisticalModelsBased1993),
    $
        W(t) = 1 + integral_0^t W(s -) ((pi_s^*)/(pi_s) - 1) M^(a 1) (dif s) + integral_0^t W(s-) ((1-pi_s^*)/(1-pi_s) - 1) M^(a 0) (dif s).
    $ <eq:sde>
    We have that
    $
        zeta_t := integral_0^t W(s -) ((pi_s^*)/(pi_s) - 1) M^(a 1) (dif s) + integral_0^t W(s-) ((1-pi_s^*)/(1-pi_s) - 1) M^(a 0) (dif s)
    $ is a zero mean $P$-$cal(H)_t$-martingale by positivity.
    From this, we see that $integral_0^t tilde(Y)_(t) zeta (dif s)$ is also a uniformly integrable $P$-$cal(H)_t$-martingale,
    which implies by Theorem 2.1.42 of @last1995marked
    $integral_0^t tilde(Y)_(t) zeta (dif s)$ is a zero mean $P$-$cal(H)_t$-martingale.
    This implies that
    $
        mean(P) [Y_t W (t)] &=^(*) mean(P) [tilde(Y)_t W (t)] = mean(P) [tilde(Y)_t] + mean(P) [integral_0^t tilde(Y)_(t) zeta (dif s)] = mean(P) [tilde(Y)_t],
    $
    where in $*$ we used consistency by noting that $W (t) != 0$ if and only if $tau^a > t$.
]

It is also natural to ask oneself: how does
our exchangeability condition
relate to the one of @ryalenPotentialOutcomes?
We present a result in this direction.

#theorem[
    Let $bb(N)_t^a = bb(1) {tau^A <= t}$.
    The exchangeability condition of @thm:identifiabilitymartingale implies the one of @ryalenPotentialOutcomes, e.g.,
    $bb(L)_t := Lambda_(t)^a$ is both the $P$-$cal(F)_(t and tau^A)$ compensator 
    and the $P$-$cal(H)_(t and tau^A)$ compensator of $bb(N)_t^a$.
]

#proof[
  Consider some localizing sequence $S_n$ for $M^(a 0)$.
  We note that $bb(N)_t^a = N^a ((0,t and tau^A], {0})$.
  Apply optional sampling to $M_(dot and S_n)^(a 0)$ at $S := tau^A and S_n and s$ and $T := t and S_n and tau^A$ to see that
  $
      mean(P) [M_(t and S_n and tau^A)^(a 0) | cal(F)_(s and tau^A)] = M_(s and S_n and tau^A)^(a 0) quad P-"a.s."
  $
  If exchangeability for $Lambda^(a 0)$ holds (given in
  @thm:identifiabilitymartingale),
  then the same argument applies with $cal(H)_t$ instead of $cal(F)_t$, so that
  $
      mean(P) [M_(t and S_n and tau^A)^(a 0) | cal(H)_(s and tau^A)] = M_(s and S_n and tau^A)^(a 0) quad P-"a.s."
  $
  This is the desired result.
]

We can also ask ourselves:
Is the exchangeability criterion in @thm:identifiabilitymartingale
close in interpretation 
to the statement of @rytgaardContinuoustimeTargetedMinimum2022?
In @rytgaardContinuoustimeTargetedMinimum2022,
the statement is:
$
    (tilde(Y)_t)_(t in [0, T]) perp A(T_(k)^a) | cal(F)^(-)_(T_(k)^a), qquad (\')
$ for all $k$, where $T_(k)^a$ are the ordered treatment event times, where $cal(F)^(-)_(T)$ is defined on p. 62 of @last1995marked.
This $sigma$-algebra contains all the information that occurs strictly before time $T$.

In this case, we can express our exchangeability condition via something
that is very similar to this statement:
$
    (tilde(Y)_t)_(t in [0, T]) perp treat(k) | status(k) = a, event(k), history(k-1), qquad (\*)
$ 
for all $k$.
The statements appear similar, but are generally not the same,
since $T$ is not generally $cal(F)_T^-$ measurable.
If $S$ and $Delta N_S^a$ are $cal(F)_S^-$ measurable, then the
two statements should be the same. 
However, if $S$ is predictable, then $S in sigma(cal(F)_S^-)$ (Theorem 2.2.19 of @last1995marked),
If $N^a_t$-predictable, it should also be the case that $Delta N_S^a in sigma(cal(F)_S^-)$?
Yes, if $N^a_t$ is predictable, then $N_t^a in sigma(cal(F)_(t-))$ (Theorem 2.2.9 of @last1995marked);
therefore $Delta N_S^a in sigma(cal(F)_S^-)$.
However, due to the classical fact that conditional independence and independence
never imply each other, the two statements are not equivalent and are generally different.
// If it does not depend on T_a, is that enough?

To have exchangeability, we also need that the compensator for $N^a = N^a (dot times {0,1})$ is the same under $cal(F)_t$ and $cal(H)_t$,
i.e., that
- $Lambda^a (dif t)$ is the $P$-$cal(F)_t$-compensator and the $P$-$cal(H)_t$-compensator of $N^a (dif t times {0,1})$. (\*\*)

A slight strengthening of $(\*)$ is that
- The Radon-Nikodym derivative of $Lambda^(a 1) (dif t)$ with respect to $Lambda^a (dif t)$
  is the same for $cal(F)_t$ and $cal(H)_t$. (\*!)
This is because there is a version of $pi_t$ such that
$
    pi_t^' = sum_k bb(1) {event(k-1) < t < event(k)} pi_(event(k)) (cal(H)_(event(k-1)))
$
We then have the following result.
#theorem[
    The conditions (\*\!) and (\*\*) hold if and only if the exchangeability condition of @thm:identifiabilitymartingale holds.
]

#proof[
To see that it is sufficient, note that (\*) and (\*\*) imply that
    $
        N^a (dif t times {x}) - (pi_t)^(bb(1) {x=1}) (1-pi_t)^(bb(1) {x=0}) Lambda^a (dif t)
    $ <eq:1>

    //in view of (\*).
    // Furthermore, a version of $Lambda^a (dif t)$ is
    // $
    //     Lambda^(a ') (dif t) = sum_(k) bb(1) {event(k-1) < t < event(k)} Lambda_k^a (dif t | cal(H)_(event(k-1)))
    // $
    //which must be both the $P$-$cal(F)_t$-compensator and the $P$-$cal(H)_t$-compensator of $N^a (dif t times {0,1})$ by (\*\*).
    //By Theorem 4.1.11 of @last1995marked, @eq:1 is a $P$-$cal(H)_t$ - local martingale.
    However, it must also be a $P$-$cal(F)_t$-local martingale;
    to see this, let $S_n$ be a localizing sequence for @eq:1 and consider $0 <= s < t$.
    Then,
    $
        mean(P) [M_(t and S_n)^(a x) | cal(F)_s] &= mean(P) [mean(P) [M_(t and S_n)^(a x)| cal(H)_s]  | cal(F)_s] \
            &= mean(P) [M_(s and S_n)^(a x) | cal(F)_s] =^(!) M_(s and S_n)^(a x) quad P-"a.s.".
    $
    In $!$, we used that $N_(s and S_n)^(a x)$ is (trivially) $cal(F)_s$ measurable;
    moreover, by (\*), $pi_s$ is $cal(F)_s$ measurable;
    finally, by (\*\*), $Lambda^(a) (dif t)$ is also the $P$-$cal(F)_t$-compensator of $N^a (dif t times {0,1})$;
    and hence it is predictable with respect to this filtration because it is $cal(F)_(t-)$ measurable (Theorem 2.2.6 of @last1995marked).
    // Nulmængder???? If we complete the filtration, then this argument should be ok.
    Conversely, to see that it is necessary, we have directly (\*\*); however this is precisely what we needed to show $(\*!)$.
]

One may ask oneself if positivity holds in @ryalenPotentialOutcomes;
under what assumptions does positivity in @thm:identifiabilitymartingale
hold?

@ryalenPotentialOutcomes introduces the weight
$
    tilde(W) = (cal(E) (-bb(N)^a)) / (cal(E) (-bb(L)^a))
$
Similarly, we can introduce the weight
$
  W = cal(E) (K),
$
where
$
    K_t = integral_0^t ( (pi_s^*)/(pi_s) - 1) N^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) N^(a 0) (dif s).
$
Can we find a process $phi$ such that
$cal(E) (K) =  (cal(E) (-bb(N)^a)) / (cal(E) (-bb(L)^a)) cal(E) (phi)$?
To this end, note that
$
    cal(E) (phi) &= (cal(E) (K) cal(E) (- bb(L)^a)) / (cal(E) (- bb(N)^a)) \
        &= (cal(E) (- bb(N)^a) cal(E) (K) cal(E) (- bb(L)^a)) / (cal(E) (- bb(N)^a)) \
        &= cal(E) (K) cal(E) (-bb(L)^a) \
        &= cal(E) (K - bb(L)^a - [K, bb(L)^a]).
$
where we use that
$
    cal(E) (K) = cal(E) (K) bb(1) {tau^a > dot} = cal(E) (K)  cal(E) (-bb(N)^a)
$
taking $0/0=1$. Note that
$
    [K, bb(L)^a]_t &= integral_0^t Delta bb(L)^a_s ((pi_s^*)/(pi_s) - 1) d N^(a 1)_s \
        &quad + integral_0^t Delta bb(L)^a_s ((1-pi_s^*)/(1-pi_s) - 1) d N^(a 0)_s \
        &=^(*) integral_0^(t and tau^a) pi_(s) Delta Lambda^a (s) ((1-pi_s^*)/(1-pi_s) - 1) d N^(a 0)_s \
        &quad + integral_0^(t and tau^a) pi_(s) Delta Lambda^a (s) ((pi_s^*)/(pi_s) - 1) d N^(a 1)_s, \
        &= integral_0^(t and tau^a) Delta Lambda^a (s) (pi_s- pi_s^*) pi_s /(1-pi_s) d N^(a 0)_s \
        &quad + integral_0^(t and tau^a) Delta Lambda^a (s) (pi_s^* - pi_s) d N^(a 1)_s, \
$

In the absolutely continuous case, $[K, bb(L)^a]_t = 0$ as $Delta Lambda^a_t = 0$ for all $t > 0$.
Also note that in $(*)$, we apply the corollary on p. 10 of @protter2005stochastic, e.g., stopped martingales
are martingales and $bb(N)_t^a = N^(a 0) (t and tau^a)$.

However, it is also the case that
$
    cal(E) (phi) &= cal(E) (K - bb(L)^a + bb(N)^a - [K, bb(L)^a])
$
because $bb(N)^a equiv 0$ whenever $cal(E) (K) != 0$ and $(cal(E) (-bb(N)^a)) / (cal(E) (-bb(L)^a)) != 0$.
Also
$
    K_t- bb(L)_t^a + bb(N)_t^a &= integral_0^(t and tau^a) ((pi_s^*)/(pi_s) - 1) M^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) M^(a 0) (dif s) \
        &+quad integral_0^(t and tau^a) 1/(1- (1-pi_s^*) Delta Lambda_s^a ) (1-pi^*_s)  (N^a (dif s) - Lambda^a (dif s)) \
        &+quad integral_0^(t and tau^a) 1/(1- pi_s^* Delta Lambda_s^a ) pi^*_s (N^a (dif s) - Lambda^a (dif s)) \
$
The last $bb(N)^a$ can be ignored.
When $pi_s^* = 1$ and $Lambda^a_s$ absolutely continuous, then

$
    K_t - bb(L)_t^a + bb(N)_t^a = integral_0^(t and tau^a) ((1)/(pi_s) - 1) M^(a 1) (dif s)
$
and $[K, bb(L)^a]_t = 0$.
// If only absolutely continuous, then
// $
//     K_t - bb(L)_t^a + bb(N)_t^a &= integral_0^(t and tau^a) ((pi^*)/(pi_s) - 1) (N^(a 1) (dif s) - pi_s Lambda^a (dif s))  + ((1-pi_s^*)/(1-pi_s) - 1) (N^(a 0) (dif s) - (1-pi_s) Lambda^a (dif s) )\
//         &+quad integral_0^(t and tau^a) (1-pi^*_s)  (N^a (dif s) - Lambda^a (dif s)) \
//         &+ quad integral_0^(t and tau^a)  pi^*_s (N^a (dif s) - Lambda^a (dif s)) \
//         &= integral_0^(t and tau^a) ((pi^*)/(pi_s) - 1) N^(a 1) (dif s) + ((1-pi_s^*)/(1-pi_s) - 1) N^(a 0) (dif s) \
//         &- quad integral_0^(t and tau^a) (pi^*) - pi_s) Lambda^a (dif s) + (pi_s-pi^*_s) Lambda^a (dif s) \
// $

#bibliography("references/ref.bib",style: "apa")
