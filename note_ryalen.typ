#import "@preview/fletcher:0.5.1": node, edge, diagram
// #import "template.typ": conf
#import "template/shortcuts.typ": *
#import "@preview/arkheion:0.1.0": arkheion
#import "@preview/ctheorems:1.1.3": *
#let definition = thmbox("definition", "Definition", inset: (x: 1.2em, top: 1em))
#let theorem = thmbox("theorem", "Theorem", fill: rgb("#eeffee"))
#import "@preview/numty:0.0.5" as nt
#set cite(form: "prose")
// Color references red
#show  ref: it => {text(fill: maroon)[#it]}

#let theorem = thmbox("theorem", "Theorem", fill: rgb("#eeffee"))
#let proof = thmproof("proof", "Proof")

#set math.equation(numbering: "(1)")
#show math.equation: it => {
  if it.block and not it.has("label") [
    #counter(math.equation).update(v => v - 1)
    #math.equation(it.body, block: true, numbering: none)#label("")
  ] else {
    it
  }  
}

#show: thmrules.with(qed-symbol: $square$)

= Potential outcomes

== Potential outcomes (martingale approach)

Let us consider the setting of @ryalenPotentialOutcomes
and their regularity conditions.
Specifically, we will work with an intervention
that specifies the treatment decisions
but not the timing of treatment visits.
We
work with Example 4 of @ryalenPotentialOutcomes, in which
$
    pi^* (phi, t, dif x) = delta_(a_0) (dif x),
$
i.e., treatment is always assigned to $a_0$.
To simplify,
we work without right-censoring and no covariates.
This means that $(N^y, N^a)$, where
$N^y$ denotes the counting process on $[0, T]$ for death
and $N^a$ random measure for treatment on
$[0, T] times {a_0, a_1}$.
For this treatment regime, we see that 
$
    tau^A = inf {t >= 0 | N^a ((0, t] times {a_1}) > 0}.
$
We can associate each of the random measures $N^y$ and $N^a$ with
the random measure
$
    N (dif (t, m, a)) = N^y (dif t) delta_y (dif m) + delta_a (dif m) {N^a (dif (t) times {a_0}) delta_(a_0) (dif m) + N^a (dif (t) times {a_1}) delta_(a_1) (dif m)}.
$
This gives rise to a counting process filtration $(cal(F)_t)_(t >= 0)$
generated by $N$. We can then find that $N$ has the compensator
$
    Lambda (dif (t, m, a)) = Lambda^y (dif t) delta_y (dif m) + delta_a (dif m) {pi_t (cal(F)_(t-)) delta_(a_0) (dif m) +  (1-pi_t (cal(F)_(t-))) delta_(a_1) (dif m)} Lambda^a (dif (t)),
$
where we can choose $pi_t$ to be $cal(F)_t$-predictable.
We are interested in the counterfactual mean outcome $mean(P) [tilde(Y)_t]$,
where $(tilde(Y)_t)_(t >= 0)$ is the counterfactual outcome process
of $Y := N^y$ under the intervention that sets treatment to $a_0$ at all visitation times.
Note the different exchangeability condition compared to @ryalenPotentialOutcomes,
as @ryalenPotentialOutcomes expresses exchangeability through the counting process $bb(1) {tau^A <= dot}$;
this is actually a weaker condition.
Let $(event(k), status(k), treat(k))$
denote the ordered event times, event types, and treatment decisions.
Note that @eq:rytgaard is the same likelihood ratio as in @rytgaardContinuoustimeTargetedMinimum2022.
We also impose the assumption that $N_t := N^y_t + N^a( {(0, t] times {a_0, a_1}})$ does not explode;
we also assume that we work with a version of the compensator
such that $Lambda ({t} times {y, a} times {a_0, a_1}) < oo$ for all $t > 0$.
We may generally also work with a compensator $Lambda$ that fulfills conditions (10.1.11)-(10.1.13) of @last1995marked.

*NOTE:* So the issue is that *Positivity* might not actually hold. If we look at $W (t)$, then it is piecewise constant and only jumps at the treatment times.
If it were generally a likelihood ratio, then it would solve @eq:sde.
For the second term, this implies that $Lambda^a (dif t) = sum_k delta_(event(k)) (dif t)$,
so we have placed restrictions on the compensator for $N^a$.
To see this note that,
$
    W(t) &= 1 + integral_0^t W(s -) V(s, m, a) (N (d(s,m, a)) - Lambda (d(s,m, a)) \
        &= 1 + integral_0^t W(s -) V(s, m, 0) (N^(a 0) (d s) - pi_s Lambda^a (d s)) \
        &qquad +  integral_0^t W(s -) V(s, m, 1) (N^(a 1) (d s) - (1-pi_s) Lambda^a (d s)) \
        &= 1 + integral_0^t W(s -) V(s, m, 0) (N^(a 0) (d s) - pi_s Lambda^(a,c) (d s)) \
        &qquad + integral_0^t W(s -) V(s, m, 1) (N^(a 1) (d s) - (1-pi_s) Lambda^(a,c) (d s)) \
        & + integral_0^t W(s -) V(s, m, 0) ( - pi_s Lambda^(a,"discrete") (d s)) \
        & + integral_0^t W(s -) V(s, m, 1) ( - (1-pi_s) Lambda^(a,"discrete") (d s)).
$
By rearranging, we therefore have that
$
    W(t) - 1 - integral_0^t W(s -) V(s, m, 0) (N^(a 0) (d s) - pi_s Lambda^(a,"discrete") (d s)) \
        - integral_0^t W(s -) V(s, m, 1) (N^(a 1) (d s) - (1-pi_s) Lambda^(a,"discrete") (d s)) \
        = -integral_0^t W(s -) (V(s, m, 0) pi_s + V(s, m, 1) (1-pi_s)) lambda^(a,c) (s) dif s
$
However, the last integral is only piecewise constant if $lambda^(a,c) equiv 0$
because $W(s -) (V(s, m, 0) pi_s + V(s, m, 1) (1-pi_s)) != 0$ unless the treatment decision is deterministic.
Consequently,
$
    W(t) - 1 - integral_0^t W(s -) V(s, m, 0) N^(a 0) (d s) \
        - integral_0^t W(s -) V(s, m, 1) N^(a 1) (d s) \
        = integral_0^t W(s -) ((V(s, m, 0) pi_s + V(s, m, 1) (1-pi_s)) Lambda^(a,"discrete") (d s)
$ <eq:piecewiseconstant>
In order for @eq:piecewiseconstant to hold true,
it must be the case that
$
    &Delta W(t) - W(t -) V(t, m, 0) Delta N^(a 0) (t) - W(t -) V(t, m, 1) Delta N^(a 1) (t) \
        &= W(t -) ((V(t, m, 0) pi_t + V(t, m, 1) (1-pi_t)) Delta Lambda^(a,"discrete") (t).
$
However, the left-hand side is only non-zero whenever $Delta N^(a) (t) != 0$;
because, again, $W(t -) ((V(t, m, 0) pi_t + V(t, m, 1) (1-pi_t)) != 0$,
we have must that $Delta Lambda^(a,"discrete") (t) = 0$ whenever $Delta N^(a) (t) = 0$.
Letting $T^a_((1)), T^a_((2)), dots$ denote the ordered jump times of $N^a$,
we thus have
$
    Lambda^a (dif t) = sum_k A_k (cal(F)_(t-)) delta_(T^a_((k))) (dif t), A_k in (0, 1]
$
Hence
$
    N^a (t) - Lambda^a (t) = sum_k (1 - A_k) bb(1) {T^a_((k)) <= t} >= 0.
$
Let $K$ be the last number such that $P(T^a_((K)) < oo) > 0$.
Note that
$
    N^a (t) - Lambda^a (t) = sum_(k=1)^K (1 - A_k) bb(1) {T^a_((k)) <= t}
$
However, the above must also be a zero mean martingale, so
that $mean(P) [(1 - A_k) bb(1) {T^a_((k)) <= t}] = 0$.
Measure theory implies that $(1- A_k ) P(T^a_((k)) <= t | A_k) = 0 quad P$-a.s. for all $t > 0$.

If $P(T^a_((k)) <= t | A_k) = 0$ for all $t>0$ almost surely
then $P(T^a_((k)) < oo | A_k) = 0$ almost surely and so $T^a_((k)) = oo$ almost surely -- a contradiction.
Therefore $A_k = 1$ for all $k = 1, dots, K$
and $N^a$ is then its own compensator.

Then, the continuous part is zero of the compensator of $a$.
In that case, local independence cannot even motivate this estimand.
// If it is well defined, then PÃ¥l actually shows that they are not the same g-formula,
// Unless the treatment counting process is predictable 

- *What about pointwise identification*?
- $mean(P) [W (tau)] = 1$ but not necessarily $mean(P) [W (t)] != 1$ for all $t$, so that $W (t)$ is not generally a martingale,
  then it still be possible to reweight as follows $mean(P) [tilde(Y)_tau] = mean(P) [Y_tau W (tau)]$.
  Might lose causal interpretation. 

- *Might be relevant:* https://pmc.ncbi.nlm.nih.gov/articles/PMC3857358/pdf/nihms529556.pdf

#theorem[
Define
$
    zeta (t, m, a) := bb(1) {m=y} + bb(1) {m=a} (bb(1) {a = a_0})/(pi_t)
$
If _all_ of the following conditions hold:
- *Consistency*: $tilde(Y)_(t) bb(1) {tau^A > dot} = Y_(t) bb(1) {tau^A > dot} quad P-"a.s."$
- *Exchangeability*:
  Define $cal(H)_t := cal(F)_t or sigma(tilde(Y))$.
  The $P$-$cal(F)_t$ compensator for $N^a$ is also the $P$-$cal(H)_t$ compensator.
- *Positivity*:
  $
      W_(t) := product_(j = 1)^(N_t) ((bb(1) {treat(j) = a_0}) / (pi_(event(j)) (history(j-1))))^(bb(1) {status(j) = a})
  $ <eq:rytgaard>
  fulfills that $integral_0^t W(s -) V(s, m, a) (N (d(s,m, a)) - Lambda (d(s,m, a))$ is a zero mean square-integrable, $P$-$cal(F)_t$-martingale.
Then,
$
    mean(P) [tilde(Y)_t] = mean(P) [Y_t W_(t)]
$
] <thm:identifiabilitymartingale>

#proof[
    We shall use that the likelihood ratio solves a specific stochastic differential equation.
    To this end, we define the random measure, $mu (d (t, m, a)) := zeta (t, m, a) nu (d (t, m, a))$,
    where $nu := Lambda$.
    The likelihood ratio process $L (t)$ given in (10.1.14) of @last1995marked is defined by
    $
        L(t) &= bb(1) {t < T_oo and T_oo (nu)) L_0 product_(n : event(n) <= t) zeta(event(n), status(n), treat(n)) \
             &quad product_(s <= t \ N_(s-) = N_(s)) (1-macron(nu) {s}) / (1-macron(mu) {s}) exp(integral bb(1){s <= t} (1-zeta(s,m, a)) nu^c (d (s, m, a))) \
            &+ bb(1) {t >= T_oo and T_oo (nu)} liminf_(s -> T_oo and T_oo (nu)) L(s).
    $ <eq:lrt>
    Here $T_oo := lim_n T_n$, $T_oo (nu) := inf {t >= 0 | nu ((0, t] times {y, a} times {a_0, a_1}) = oo}$,
    $macron(mu)(dot) := mu(dot times {y, a} times {a_0, a_1})$,
    $macron(nu)(dot) := nu(dot times {y, a} times {a_0, a_1})$,
    $nu^c (dif (s, m, a)) := bb(1) {macron(nu) {s} = 0} nu (d (s, m, a))$, and $L_0 := W (0) = 1$.

    By our assumptions, $T_oo = oo quad P $-a.s. and 
    thus $T_oo (nu) = T_oo = oo$ in view Theorem 4.1.7 (ii) of @last1995marked
    since $macron(nu) {t} < oo$ for all $t > 0$.
    
    Second, note that $macron(nu) = macron(mu)$. This follows since
    $
        macron(mu) (A) &= integral_(A times {y, a} times {a_0, a_1}) zeta(t, m, a) nu (d (t, m, a)) \
            &=integral_(A times {y} times {a_0, a_1}) zeta(t, m, a) nu (d (t, m, a))  + integral_(A times {a} times {a_0, a_1}) zeta(t, m, a) nu (d (t, m, a)) \
    &=integral_(A times {y} times {a_0, a_1}) 1 nu (d (t, m, a))  + integral_(A times {a} times {a_0, a_1}) ((bb(1) {a=a_0})/pi_t)  nu (d (t, m, a)) \
            &=nu (A times {y} times {a_0, a_1})  + integral_(A) Lambda^a (d t)\
            &=nu (A times {y} times {a_0, a_1}) + nu (A times {a} times {a_0, a_1}) \
            &=nu (A times {y, a} times {a_0, a_1}) = macron(nu) (A).
    $
    Thus
    $
        product_(s <= t \ N_(s-) = N_(s)) (1-macron(nu) {s}) / (1-macron(mu) {s}) exp(integral bb(1){s <= t} (1-zeta(s,m, a)) nu^c (d (s, m, a))) = 1,
    $
    and hence
    $
        L(t) &= product_(n : event(n) <= t) zeta(event(n), status(n), treat(n)) \
            &=^"def."W (t).
    $
    Let $V(s,m, a) = zeta(s,m, a) - 1 + (macron(nu) {s}-macron(mu) {s})/(1-macron(mu) {s}) = zeta(s,m, a)-1$.
    $L (t)$ will fulfill that
    $
        L (t) = L_0 + integral bb(1) {s <= t} V(s,m, a) L(s-) [Phi (d(s,m, a)) - nu (d(s,m, a))] \
    $ 
    if
    $
        mean(P) [L_0] = 1, \
        macron(mu) {t} <= 1, \
        macron(mu) {t} = 1 " if " macron(nu) {t} = 1, \
        macron(mu) [T_oo and T_oo (mu)] = 0 " and " macron(nu) [T_oo and T_oo (nu)] = 0.
    $ <eq:conditionstheorem1022>
    by Theorem 10.2.2 of @last1995marked.
    These can be easily verified.
    
    // The first condition holds by positivity. The second condition holds by the specific choice
    // of compensator since $sum_x cumhazard(k, x, {t}) <= 1$ for all $k = 1, dots, K$ and $t in (0, tauend]$ (Theorem A5.9 of @last1995marked).
    // The third holds since $macron(mu) = macron(nu)$ and the fourth holds since $T_oo = T_oo (nu) = T_oo (mu) = oo$.
    
    Thus,
    $
        W(t) = 1 + integral_0^t W(s -) V(s, m, a) (Phi (d(s,m, a)) - nu (d(s,m, a)).
    $ <eq:sde>
    and it follows that $integral_0^t W(s -) V(s, m, a) (Phi (d(s,m, a)) - nu (d(s,m, a))$ is a zero mean$P$-$cal(H)_t$-martingale.
    From this, we see that $integral_0^t tilde(Y)_(t) W_(s-) V(s, m, a) (Phi (d(s,m, a)) - nu (d(s,m, a))$ is also a zero mean $P$-$cal(H)_t$-martingale.
    This implies that
    $
        mean(P) [Y_t W_(t)] &=^"(consistency)" mean(P) [tilde(Y)_t W_t] \
            &= mean(P) [tilde(Y)_t] + mean(P) [integral_0^t tilde(Y)_(t) W_(s-) V(s, m, a) (Phi (d(s,m, a)) - nu (d(s,m, a))] \
                &= mean(P) [tilde(Y)_t].
    $
]

== Local approach

#theorem[
- *Consistency*: $tilde(Y)_(tau) bb(1) {tau^A > tau} = Y_(tau) bb(1) {tau^A > tau} quad P-"a.s."$
- *Exchangeability*: We have
   $
       &tilde(Y)_tau bb(1) {event(j) <= tau < event(j+1)}
       perp treat(k) | history(k-1), event(k), status(k) = a, quad forall j>=k>0
   $ <eq:exchangeability>
- *Positivity*: Let $pi_(event(k)) (history(k-1))$ denote the conditional probability of receiving treatment $a_0$ at time $event(k)$ given $history(k-1)$ and $status(k) = a$.
  We have $eta > 0$ such that $pi_(event(k)) (history(k-1)) > eta quad P$-a.s. for all $k >= 1$.
Then the estimand of interest is identifiable, i.e.,
$
        mean(P) [tilde(Y)_tau] = mean(P) [Y_tau W_(tau)]
$
]<thm:identifiability>
#proof[
    Write $tilde(Y)_t = sum_(k=1)^oo bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau$.
    The theorem is shown if we can prove that $mean(P) [bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau] = mean(P) [bb(1) {event(k-1) <= tau < event(k)} Y_tau W_(tau)]$
    by linearity of expectation.
    We have that for $k >= 1$,
    $
        &bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} Y_tau W(tau)] \
            &= bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} bb(1) {tau^A > tau} Y_tau W(tau)] \
            &=bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} bb(1) {tau^A > tau} tilde(Y)_tau W(tau)] \
            &=bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau W(tau)] \
            &=bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau W(event(k-1)) ]\
            &=bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau ((bb(1) {treat(k-1) = 1}) / (pi_(event(k-1)) (history(k-2))))^(bb(1) {status(k-1) =a}) W(event(k-2)) ]\
            &=bb(E)_P [ bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau | history(k-2), status(k-1), event(k-1), treat(k-1)] \
                &qquad times ((bb(1) {treat(k-1) = 1}) / (pi_(event(k-1)) (history(k-2))))^(bb(1) {status(k-1) =a}) W(event(k-2)) ]\
                        &=bb(E)_P [ bb(E)_P [ bb(1) {event(k-1) <= tau< event(k)} tilde(Y)_tau | history(k-2), status(k-1), event(k-1)] \
                &qquad times ((bb(1) {treat(k-1) = 1}) / (pi_(event(k-1)) (history(k-2))))^(bb(1) {status(k-1) =a}) W(event(k-2)) ]\
            &=bb(E)_P [ bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau | history(k-2), status(k-1), event(k-1)] \
                &qquad times mean(P) [((bb(1) {treat(k-1) = 1}) / (pi_(event(k-1)) (history(k-2))))^(bb(1) {status(k-1) =a}) | history(k-2), status(k-1), event(k-1)] W(event(k-2)) ]\
            &=bb(E)_P [ bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau | history(k-2), status(k-1), event(k-1)] W(event(k-2)) ]\
            &=bb(E)_P [ bb(E)_P [ bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau | history(k-3), status(k-2), event(k-2), treat(k-2)] W(event(k-2)) ]\
    $
    Iteratively applying the same argument, we get that
    $bb(E)_P [  bb(1) {event(k-1) <= tau < event(k)} tilde(Y)_tau ] = bb(E)_P [  bb(1) {event(k-1) <= tau < event(k)} Y_tau W(tau)]$ as needed.
]


#bibliography("references/ref.bib",style: "apa")


