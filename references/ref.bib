@article{rytgaardContinuoustimeTargetedMinimum2022,
  title = {Continuous-Time Targeted Minimum Loss-Based Estimation of Intervention-Specific Mean Outcomes},
  author = {Rytgaard, Helene C. and Gerds, Thomas A. and {van der Laan}, Mark J.},
  year = {2022},
  month = oct,
  journal = {The Annals of Stat istics},
  volume = {50},
  number = {5},
  pages = {2469--2491},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/21-AOS2114},
  urldate = {2024-01-16},
  file = {/home/johan/Zotero/storage/M8NHB426/Rytgaard et al. - 2022 - Continuous-time targeted minimum loss-based estima.pdf}
}

@book{last1995marked,
  title={Marked Point Processes on the Real Line: The Dynamical Approach},
  author={Last, G. and Brandt, A.},
  isbn={9780387945477},
  lccn={95018596},
  series={Probability and Its Applications},
  url={https://link.springer.com/book/9780387945477},
  year={1995},
  publisher={Springer}
}

@misc{sunRoleDiscretizationScales2023,
  title = {The Role of Discretization Scales in Causal Inference with Continuous-Time Treatment},
  author = {Sun, Jinghao and Crawford, Forrest W.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08840},
  eprint = {2306.08840},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.08840},
  urldate = {2024-02-15},
  abstract = {There are well-established methods for identifying the causal effect of a time-varying treatment applied at discrete time points. However, in the real world, many treatments are continuous or have a finer time scale than the one used for measurement or analysis. While researchers have investigated the discrepancies between estimates under varying discretization scales using simulations and empirical data, it is still unclear how the choice of discretization scale affects causal inference. To address this gap, we present a framework to understand how discretization scales impact the properties of causal inferences about the effect of a time-varying treatment. We introduce the concept of "identification bias", which is the difference between the causal estimand for a continuous-time treatment and the purported estimand of a discretized version of the treatment. We show that this bias can persist even with an infinite number of longitudinal treatment-outcome trajectories. We specifically examine the identification problem in a class of linear stochastic continuous-time data-generating processes and demonstrate the identification bias of the g-formula in this context. Our findings indicate that discretization bias can significantly impact empirical analysis, especially when there are limited repeated measurements. Therefore, we recommend that researchers carefully consider the choice of discretization scale and perform sensitivity analysis to address this bias. We also propose a simple and heuristic quantitative measure for sensitivity concerning discretization and suggest that researchers report this measure along with point and interval estimates in their work. By doing so, researchers can better understand and address the potential impact of discretization bias on causal inference.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  file = {/home/johan/Zotero/storage/GCQCSPLE/Sun and Crawford - 2023 - The role of discretization scales in causal infere.pdf;/home/johan/Zotero/storage/2UB5BF7I/2306.html}
}

@book{pearlCausalityModelsReasoning2009,
  title = {Causality: {{Models}}, {{Reasoning}} and {{Inference}}},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  month = aug,
  edition = {2nd},
  publisher = {Cambridge University Press},
  address = {USA},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  isbn = {978-0-521-89560-6}
}

@book{andersenStatisticalModelsBased1993,
  title = {Statistical {{Models Based}} on {{Counting Processes}}},
  author = {Andersen, Per Kragh and Borgan, {\O}rnulf and Gill, Richard D. and Keiding, Niels},
  year = {1993},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer US},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4348-9},
  urldate = {2024-03-12},
  isbn = {978-0-387-94519-4 978-1-4612-4348-9},
  keywords = {censoring,estimator,likelihood,survival analysis},
  file = {/home/johan/Zotero/storage/M2EFUKWS/Andersen et al. - 1993 - Statistical Models Based on Counting Processes.pdf}
}

@misc{shirakawaLongitudinalTargetedMinimum2024,
  title = {Longitudinal {{Targeted Minimum Loss-based Estimation}} with {{Temporal-Difference Heterogeneous Transformer}}},
  author = {Shirakawa, Toru and Li, Yi and Wu, Yulun and Qiu, Sky and Li, Yuxuan and Zhao, Mingduo and Iso, Hiroyasu and {van der Laan}, Mark},
  year = {2024},
  month = apr,
  number = {arXiv:2404.04399},
  eprint = {2404.04399},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-04},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/johan/Zotero/storage/A85PMYRL/Shirakawa et al. - 2024 - Longitudinal Targeted Minimum Loss-based Estimatio.pdf}
}

@misc{schulerSelectivelyAdaptiveLasso2022,
  title = {The {{Selectively Adaptive Lasso}}},
  author = {Schuler, Alejandro and {van der Laan}, Mark},
  year = {2022},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-10-03},
  howpublished = {https://arxiv.org/abs/2205.10697v1},
  langid = {english},
  file = {/home/johan/Zotero/storage/CY39YPCL/Schuler and van der Laan - 2022 - The Selectively Adaptive Lasso.pdf}
}

@misc{liguoriModelingEventsInteractions2023,
  title = {Modeling {{Events}} and {{Interactions}} through {{Temporal Processes}} -- {{A Survey}}},
  author = {Liguori, Angelica and Caroprese, Luciano and Minici, Marco and Veloso, Bruno and Spinnato, Francesco and Nanni, Mirco and Manco, Giuseppe and Gama, Joao},
  year = {2023},
  month = jul,
  number = {arXiv:2303.06067},
  eprint = {2303.06067},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06067},
  urldate = {2024-05-15},
  abstract = {In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/TJN7L7VQ/Liguori et al. - 2023 - Modeling Events and Interactions through Temporal .pdf;/home/johan/Zotero/storage/ZU7AZWY7/2303.html}
}

@inproceedings{weissForestBasedPointProcess2013,
  title = {Forest-{{Based Point Process}} for {{Event Prediction}} from {{Electronic Health Records}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Weiss, Jeremy C. and Page, David},
  editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
  year = {2013},
  pages = {547--562},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40994-3_35},
  abstract = {Accurate prediction of future onset of disease from Electronic Health Records (EHRs) has important clinical and economic implications. In this domain the arrival of data comes at semi-irregular intervals and makes the prediction task challenging. We propose a method called multiplicative-forest point processes (MFPPs) that learns the rate of future events based on an event history. MFPPs join previous theory in multiplicative forest continuous-time Bayesian networks and piecewise-continuous conditional intensity models. We analyze the advantages of using MFPPs over previous methods and show that on synthetic and real EHR forecasting of heart attacks, MFPPs outperform earlier methods and augment off-the-shelf machine learning algorithms.},
  isbn = {978-3-642-40994-3},
  langid = {english},
  file = {/home/johan/Zotero/storage/I4XYNKSV/Weiss and Page - 2013 - Forest-Based Point Process for Event Prediction fr.pdf}
}

@article{rubinEstimatingCausalEffects2005,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590541},
 abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {469},
 pages = {322--331},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Causal Inference Using Potential Outcomes: Design, Modeling, Decisions},
 urldate = {2024-10-09},
 volume = {100},
 year = {2005}
}

@article{RobinsLongitudinal2001,
author = {Richard D. Gill and James M. Robins},
title = {{Causal Inference for Complex Longitudinal Data: The Continuous Case}},
volume = {29},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {1785 -- 1811},
keywords = {causality, counterfactuals, longitudinal data, observational studies},
year = {2001},
doi = {10.1214/aos/1015345962},
URL = {https://doi.org/10.1214/aos/1015345962}
}

@article{robins1986,
title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
journal = {Mathematical Modelling},
volume = {7},
number = {9},
pages = {1393-1512},
year = {1986},
issn = {0270-0255},
doi = {https://doi.org/10.1016/0270-0255(86)90088-6},
url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
author = {James Robins},
abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.}
}

@article{chernozhukovDoubleMachineLearning2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@article{laanTargetedMaximumLikelihood2006,
  title = {Targeted {{Maximum Likelihood Learning}}},
  author = {{van der Laan}, Mark J. and Rubin, Daniel},
  year = {2006},
  month = dec,
  journal = {The International Journal of Biostatistics},
  volume = {2},
  number = {1},
  publisher = {De Gruyter},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1043},
  urldate = {2024-01-02},
  abstract = {Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution. Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology. We illustrate the method with various worked out examples.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {causal effect,cross-validation,efficient influence curve,estimating function,locally efficient estimation,loss function,maximum likelihood estimation,sieve,targeted maximum likelihood estimation,variable importance},
  file = {/home/johan/Zotero/storage/2TAEU6FE/Laan and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf}
}

@article{laanTargetedMinimumLoss2012,
  title = {Targeted {{Minimum Loss Based Estimation}} of {{Causal Effects}} of {{Multiple Time Point Interventions}}},
  author = {{van der Laan}, Mark J. and Gruber, Susan},
  year = {2012},
  month = may,
  journal = {The International Journal of Biostatistics},
  volume = {8},
  number = {1},
  publisher = {De Gruyter},
  issn = {1557-4679},
  doi = {10.1515/1557-4679.1370},
  urldate = {2024-01-02},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {Asymptotic linearity of an estimator,causal effect,confounding,efficient influence curve,G-computation formula,influence curve,longitudinal data,loss function,marginal structural working model,nonparametric structural equation model,positivity assumption,randomization assumption,semiparametric statistical model,targeted maximum likelihood estimation,targeted minimum loss based estimation,TMLE,treatment regimen},
  file = {/home/johan/Zotero/storage/HFG2787X/Laan and Gruber - 2012 - Targeted Minimum Loss Based Estimation of Causal E.pdf}
}

@article{discretizationGuerra2020,
author = {Ferreira Guerra, Steve and Schnitzer, Mireille E. and Forget, Amélie and Blais, Lucie},
title = {Impact of discretization of the timeline for longitudinal causal inference methods},
journal = {Statistics in Medicine},
volume = {39},
number = {27},
pages = {4069-4085},
keywords = {coarsening, cross-validation, electronic health data, semiparametric estimation, TMLE},
doi = {https://doi.org/10.1002/sim.8710},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8710},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8710},
abstract = {In longitudinal settings, causal inference methods usually rely on a discretization of the patient timeline that may not reflect the underlying data generation process. This article investigates the estimation of causal parameters under discretized data. It presents the implicit assumptions practitioners make but do not acknowledge when discretizing data to assess longitudinal causal parameters. We illustrate that differences in point estimates under different discretizations are due to the data coarsening resulting in both a modified definition of the parameter of interest and loss of information about time-dependent confounders. We further investigate several tools to advise analysts in selecting a timeline discretization for use with pooled longitudinal targeted maximum likelihood estimation for the estimation of the parameters of a marginal structural model. We use a simulation study to empirically evaluate bias at different discretizations and assess the use of the cross-validated variance as a measure of data support to select a discretization under a chosen data coarsening mechanism. We then apply our approach to a study on the relative effect of alternative asthma treatments during pregnancy on pregnancy duration. The results of the simulation study illustrate how coarsening changes the target parameter of interest as well as how it may create bias due to a lack of appropriate control for time-dependent confounders. We also observe evidence that the cross-validated variance acts well as a measure of support in the data, by being minimized at finer discretizations as the sample size increases.},
year = {2020}
}

@Inbook{Rose2011,
author="Rose, Sherri and {van der Laan}, Mark J.",
title="Introduction to TMLE",
bookTitle="Targeted Learning: Causal Inference for Observational and Experimental Data",
year="2011",
publisher="Springer New York",
address="New York, NY",
pages="67--82",
abstract="This is the second chapter in our text to deal with estimation.We started by defining the research question. This included our data, model for the probability distribution that generated the data, and the target parameter of the probability distribution of the data. We then presented the estimation of prediction functions using super learning. This leads us to the estimation of causal effects using the TMLE. This chapter introduces TMLE, and a deeper understanding of this methodology is provided in Chap. 5. Note that we use the abbreviation TMLE for targeted maximum likelihood estimation and the targeted maximum likelihood estimator. Later in this text, we discuss targeted minimum loss-based estimation, which can also be abbreviated TMLE.",
isbn="978-1-4419-9782-1",
doi="10.1007/978-1-4419-9782-1_4",
url="https://doi.org/10.1007/978-1-4419-9782-1_4"
}

@book{jacobsen2006point,
  title={Point Process Theory and Applications: Marked Point and Piecewise Deterministic Processes},
  author={Jacobsen, M.},
  isbn={9780817644635},
  lccn={2005934409},
  series={Probability and Its Applications},
  url={https://doi.org/10.1007/0-8176-4463-6},
  year={2006},
  publisher={Birkh{\"a}user Boston}
}

@article{roeysland2024,
    author = {Røysland, Kjetil and C. Ryalen, Pål and Nygård, Mari and Didelez, Vanessa},
    title = {Graphical criteria for the identification of marginal causal effects in continuous-time survival and event-history analyses},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    pages = {qkae056},
    year = {2024},
    month = {07},
    abstract = {We consider continuous-time survival and event-history settings, where our aim is to graphically represent causal structures allowing us to characterize when a causal parameter is identified from observational data. This causal parameter is formalized as the effect on an outcome event of a (possibly hypothetical) intervention on the intensity of a treatment process. To establish identifiability, we propose novel graphical rules indicating whether the observed information is sufficient to obtain the desired causal effect by suitable reweighting. This requires a different type of graph than in discrete time. We formally define causal semantics for the corresponding dynamic graphs that represent local independence models for multivariate counting processes. Importantly, our work highlights that causal inference from censored data relies on subtle structural assumptions on the censoring process beyond independent censoring; these can be verified graphically. Put together, our results are the first to establish graphical rules for nonparametric causal identifiability in event processes in this generality for the continuous-time case, not relying on particular parametric survival models. We conclude with a data example on Human papillomavirus (HPV) testing for cervical cancer screening, where the assumptions are illustrated graphically and the desired effect is estimated by reweighted cumulative incidence curves.},
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae056},
    url = {https://doi.org/10.1093/jrsssb/qkae056},
    eprint = {https://academic.oup.com/jrsssb/advance-article-pdf/doi/10.1093/jrsssb/qkae056/58499481/qkae056.pdf},
}


@article{van2010collaborative,
  title={Collaborative double robust targeted maximum likelihood estimation},
  author={{van der Laan}, Mark J and Gruber, Susan},
  journal={The international journal of biostatistics},
  volume={6},
  number={1},
  year={2010},
  publisher={De Gruyter}
}

@article{hubbard2016statistical,
  title={Statistical inference for data adaptive target parameters},
  author={Hubbard, Alan E and Kherad-Pajouh, Sara and {van der Laan}, Mark J},
  journal={The international journal of biostatistics},
  volume={12},
  number={1},
  pages={3--19},
  year={2016},
  publisher={De Gruyter}
}


@misc{gill2023causalinferencecomplexlongitudinal,
      title={Causal Inference for Complex Longitudinal Data: The Continuous Time g-Computation Formula}, 
      author={R. D. Gill and J. M. Robins},
      year={2023},
      eprint={math/0409436},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/math/0409436}, 
}

@article{ryalenPotentialOutcomes,
      title = {On the role of martingales in continuous-time causal inference},
      author={Pål Ryalen},
      year={2024},
      publisher={Unpublished},
}

@book{cohen2015stochastic,
  title={Stochastic calculus and applications},
  author={Cohen, Samuel N and Elliott, Robert James},
  volume={2},
  year={2015},
  publisher={Springer}
}

@thesis{chamapiwa2018application,
  title={Application of Marginal Structural Models (MSMs) to Irregular Data Settings},
  author={Chamapiwa, Edmore},
  year={2018},
  publisher={The University of Manchester (United Kingdom)}
}

@article{localindependence,
author = {Didelez, Vanessa},
title = {Graphical models for marked point processes based on local independence},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {70},
number = {1},
pages = {245-264},
keywords = {Conditional independence, Counting processes, Event history analysis, Granger causality, Graphoid, Multistate models},
doi = {https://doi.org/10.1111/j.1467-9868.2007.00634.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00634.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00634.x},
abstract = {Summary.  A new class of graphical models capturing the dependence structure of events that occur in time is proposed. The graphs represent so-called local independences, meaning that the intensities of certain types of events are independent of some (but not necessarilly all) events in the past. This dynamic concept of independence is asymmetric, similar to Granger non-causality, so the corresponding local independence graphs differ considerably from classical graphical models. Hence a new notion of graph separation, which is called δ-separation, is introduced and implications for the underlying model as well as for likelihood inference are explored. Benefits regarding facilitation of reasoning about and understanding of dynamic dependences as well as computational simplifications are discussed.},
year = {2008}
}

@book{bickel1993efficient,
  title={Efficient and adaptive estimation for semiparametric models},
  author={Bickel, Peter J and Klaassen, Chris AJ and Bickel, Peter J and Ritov, Ya’acov and Klaassen, J and Wellner, Jon A and Ritov, YA'Acov},
  volume={4},
  year={1993},
  publisher={Johns Hopkins University Press Baltimore}
}

@article{chernozhukov2018double,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = {Double/debiased machine learning for treatment and structural parameters},
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@article{jacod1975multivariate,
  title={Multivariate point processes: predictable projection, Radon-Nikodym derivatives, representation of martingales},
  author={Jacod, Jean},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
  volume={31},
  number={3},
  pages={235--253},
  year={1975},
  publisher={Springer}
}

@article{gill1990survey,
  title={A survey of product-integration with a view toward application in survival analysis},
  author={Gill, Richard D and Johansen, S{\o}ren},
  journal={The annals of statistics},
  pages={1501--1555},
  year={1990},
  publisher={JSTOR}
}

@article{ltmle,
url = {https://doi.org/10.1515/1557-4679.1370},
title = {Targeted Minimum Loss Based Estimation of Causal Effects of Multiple Time Point Interventions},
title = {},
author = {{van der Laan}, Mark J. and Gruber, Susan},
volume = {8},
number = {1},
journal = {The International Journal of Biostatistics},
doi = {doi:10.1515/1557-4679.1370},
year = {2012},
lastchecked = {2025-04-15}
}

@article{onesteptmle,
    author = {Cai, Weixin and van der Laan, Mark J.},
    title = {One-Step Targeted Maximum Likelihood Estimation for Time-to-Event Outcomes},
    journal = {Biometrics},
    volume = {76},
    number = {3},
    pages = {722-733},
    year = {2019},
    month = {11},
    abstract = {Researchers in observational survival analysis are interested in not only estimating survival curve nonparametrically but also having statistical inference for the parameter. We consider right-censored failure time data where we observe n independent and identically distributed observations of a vector random variable consisting of baseline covariates, a binary treatment at baseline, a survival time subject to right censoring, and the censoring indicator. We assume the baseline covariates are allowed to affect the treatment and censoring so that an estimator that ignores covariate information would be inconsistent. The goal is to use these data to estimate the counterfactual average survival curve of the population if all subjects are assigned the same treatment at baseline. Existing observational survival analysis methods do not result in monotone survival curve estimators, which is undesirable and may lose efficiency by not constraining the shape of the estimator using the prior knowledge of the estimand. In this paper, we present a one-step Targeted Maximum Likelihood Estimator (TMLE) for estimating the counterfactual average survival curve. We show that this new TMLE can be executed via recursion in small local updates. We demonstrate the finite sample performance of this one-step TMLE in simulations and an application to a monoclonal gammopathy data.},
    issn = {0006-341X},
    doi = {10.1111/biom.13172},
    url = {https://doi.org/10.1111/biom.13172},
    eprint = {https://academic.oup.com/biometrics/article-pdf/76/3/722/54279272/biometrics\_76\_3\_722.pdf},
}


@article{westling2024inference,
  title={Inference for treatment-specific survival curves using machine learning},
  author={Westling, Ted and Luedtke, Alex and Gilbert, Peter B and Carone, Marco},
  journal={Journal of the American Statistical Association},
  volume={119},
  number={546},
  pages={1541--1553},
  year={2024},
  publisher={Taylor \& Francis}
}

@article{richardson2013single,
  title={Single world intervention graphs (SWIGs): A unification of the counterfactual and graphical approaches to causality},
  author={Richardson, Thomas S and Robins, James M},
  journal={Center for the Statistics and the Social Sciences, University of Washington Series. Working Paper},
  volume={128},
  number={30},
  pages={2013},
  year={2013},
  publisher={Citeseer}
}

@article{andersen2003pseudovalue,
    author = {Andersen, Per Kragh and Klein, John P. and Rosthøj, Susanne},
    title = {Generalised linear models for correlated pseudo‐observations, with applications to multi‐state models},
    journal = {Biometrika},
    volume = {90},
    number = {1},
    pages = {15-27},
    year = {2003},
    month = {03},
    abstract = {In multi‐state models regression analysis typically involves the modelling of each transition intensity separately. Each probability of interest, namely the probability that a subject will be in a given state at some time, is a complex nonlinear function of the intensity regression coefficients. We present a technique which models the state probabilities directly. This method is based on the pseudo‐values from a jackknife statistic constructed from simple summary statistic estimates of the state probabilities. These pseudo‐values are then used in a generalised estimating equation to obtain estimates of the model parameters. We illustrate how this technique works by studying examples of common regression problems. We apply the technique to model acute graft‐versus‐host disease in bone marrow transplants.},
    issn = {0006-3444},
    doi = {10.1093/biomet/90.1.15},
    url = {https://doi.org/10.1093/biomet/90.1.15},
    eprint = {https://academic.oup.com/biomet/article-pdf/90/1/15/582179/900015.pdf},
}

@article{roysland2011martingale,
  title={A martingale approach to continuous-time marginal structural models},
  author={R{\o}ysland, Kjetil},
  year={2011}
}

@article{lok2008,
author = {Judith J. Lok},
title = {{Statistical modeling of causal effects in continuous time}},
volume = {36},
journal = {The Annals of Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {1464 -- 1507},
keywords = {Causality in continuous time, counterfactuals, longitudinal data, observational studies},
year = {2008},
doi = {10.1214/009053607000000820},
URL = {https://doi.org/10.1214/009053607000000820}
}

@article{cox1972,
author = {Cox, D. R.},
title = {Regression Models and Life-Tables},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {34},
number = {2},
pages = {187-202},
keywords = {life table, hazard function, age-specific failure rate, product limit estimate, regression, conditional inference, asymptotic theory, censored data, two-sample rank tests, medical applications, reliability theory, accelerated life tests},
doi = {https://doi.org/10.1111/j.2517-6161.1972.tb00899.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1972.tb00899.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1972.tb00899.x},
abstract = {Summary The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
year = {1972}
}

@inproceedings{aalen1980model,
  title={A model for nonparametric regression analysis of counting processes},
  author={Aalen, Odd},
  booktitle={Mathematical Statistics and Probability Theory: Proceedings, Sixth International Conference, Wis{\l}a (Poland), 1978},
  pages={1--25},
  year={1980},
  organization={Springer}
}

@article{ryalen2019additive,
  title={The additive hazard estimator is consistent for continuous-time marginal structural models},
  author={Ryalen, P{\aa}l C and Stensrud, Mats J and R{\o}ysland, Kjetil},
  journal={Lifetime data analysis},
  volume={25},
  pages={611--638},
  year={2019},
  publisher={Springer}
}

@article{discretization2020guerra,
author = {Ferreira Guerra, Steve and Schnitzer, Mireille E. and Forget, Amélie and Blais, Lucie},
title = {Impact of discretization of the timeline for longitudinal causal inference methods},
journal = {Statistics in Medicine},
volume = {39},
number = {27},
pages = {4069-4085},
keywords = {coarsening, cross-validation, electronic health data, semiparametric estimation, TMLE},
doi = {https://doi.org/10.1002/sim.8710},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8710},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8710},
abstract = {In longitudinal settings, causal inference methods usually rely on a discretization of the patient timeline that may not reflect the underlying data generation process. This article investigates the estimation of causal parameters under discretized data. It presents the implicit assumptions practitioners make but do not acknowledge when discretizing data to assess longitudinal causal parameters. We illustrate that differences in point estimates under different discretizations are due to the data coarsening resulting in both a modified definition of the parameter of interest and loss of information about time-dependent confounders. We further investigate several tools to advise analysts in selecting a timeline discretization for use with pooled longitudinal targeted maximum likelihood estimation for the estimation of the parameters of a marginal structural model. We use a simulation study to empirically evaluate bias at different discretizations and assess the use of the cross-validated variance as a measure of data support to select a discretization under a chosen data coarsening mechanism. We then apply our approach to a study on the relative effect of alternative asthma treatments during pregnancy on pregnancy duration. The results of the simulation study illustrate how coarsening changes the target parameter of interest as well as how it may create bias due to a lack of appropriate control for time-dependent confounders. We also observe evidence that the cross-validated variance acts well as a measure of support in the data, by being minimized at finer discretizations as the sample size increases.},
year = {2020}
}

@book{protter2005stochastic,
  title={Stochastic differential equations},
  author={Protter, Philip E},
  year={2005},
  publisher={Springer}

}

@Inbook{gill1994,
author="Gill, Richard D.",
editor="Bernard, Pierre",
title="Lectures on survival analysis",
bookTitle="Lectures on Probability Theory: Ecole d'Et{\'e} de Probabilit{\'e}s de Saint-Flour XXII-1992",
year="1994",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="115--241",
isbn="978-3-540-48568-1",
doi="10.1007/BFb0073873",
url="https://doi.org/10.1007/BFb0073873"
}

@article{onrobinsformula,
url = {https://doi.org/10.1524/stnd.22.3.171.57065},
title = {On Robins’ formula},
author = {Aad {van der Vaart}},
pages = {171--200},
volume = {22},
number = {3},
journal = {Statistics & Decisions},
doi = {doi:10.1524/stnd.22.3.171.57065},
year = {2004},
lastchecked = {2025-05-22}
}

@book{vaart1998, place={Cambridge}, series={Cambridge Series in Statistical and Probabilistic Mathematics}, title={Asymptotic Statistics}, publisher={Cambridge University Press}, author={{van der Vaart}, A. W. }, year={1998}, collection={Cambridge Series in Statistical and Probabilistic Mathematics}}

@article{bangandrobins2005,
    author = {Bang, Heejung and Robins, James M.},
    title = {Doubly Robust Estimation in Missing Data and Causal Inference Models},
    journal = {Biometrics},
    volume = {61},
    number = {4},
    pages = {962-973},
    year = {2005},
    month = {12},
    abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
    issn = {0006-341X},
    doi = {10.1111/j.1541-0420.2005.00377.x},
    url = {https://doi.org/10.1111/j.1541-0420.2005.00377.x},
    eprint = {https://academic.oup.com/biometrics/article-pdf/61/4/962/51974493/biometrics\_61\_4\_962.pdf},
}

@inproceedings{gill1997coarsening,
  title={Coarsening at random: Characterizations, conjectures, counter-examples},
  author={Gill, Richard D. and {van der Laan}, Mark J. and Robins, James M.},
  booktitle={Proceedings of the First Seattle Symposium in Biostatistics: Survival Analysis},
  pages={255--294},
  year={1997},
  organization={Springer}
}


@misc{schuler2023lassoedtreeboosting,
      title={Lassoed Tree Boosting}, 
      author={Alejandro Schuler and Yi Li and Mark van der Laan},
      year={2023},
      eprint={2205.10697},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2205.10697}, 
}

@article{kant2025irregular,
  title={Irregular measurement times in estimating time-varying treatment effects: Categorizing biases and comparing adjustment methods},
  author={Kant, Wouter MR and Krijthe, Jesse H},
  journal={arXiv preprint arXiv:2501.11449},
  year={2025}
}

@article{sun2023role,
  title={The role of discretization scales in causal inference with continuous-time treatment},
  author={Sun, Jinghao and Crawford, Forrest W},
  journal={arXiv preprint arXiv:2306.08840},
  year={2023}
}

@article{adams2020impact,
  title={The impact of time series length and discretization on longitudinal causal estimation methods},
  author={Adams, Roy and Saria, Suchi and Rosenblum, Michael},
  journal={arXiv preprint arXiv:2011.15099},
  year={2020}
}

@article{sofrygin2019targeted,
  title={Targeted learning with daily EHR data},
  author={Sofrygin, Oleg and Zhu, Zheng and Schmittdiel, Julie A and Adams, Alyce S and Grant, Richard W and {van der Laan}, Mark J and Neugebauer, Romain},
  journal={Statistics in medicine},
  volume={38},
  number={16},
  pages={3073--3090},
  year={2019},
  publisher={Wiley Online Library}
}

@book{tsiatis2006semiparametric,
  title={Semiparametric theory and missing data},
  author={Tsiatis, Anastasios A},
  year={2006},
  publisher={Springer}
}

@article{ogata1981lewis,
  title={On Lewis' simulation method for point processes},
  author={Ogata, Yosihiko},
  journal={IEEE transactions on information theory},
  volume={27},
  number={1},
  pages={23--31},
  year={1981},
  publisher={IEEE}
}

@article{lasso,
 ISSN = 00359246,
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = 1,
 pages = {267--288},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Regression Shrinkage and Selection via the Lasso},
 urldate = {2025-07-03},
 volume = 58,
 year = 1996
}

@misc{ohlendorff2025cheapsubsamplingbootstrapconfidence,
      title={Cheap Subsampling bootstrap confidence intervals for fast and robust inference}, 
      author={Johan Sebastian Ohlendorff and Anders Munch and Kathrine Kold Sørensen and Thomas Alexander Gerds},
      year={2025},
      eprint={2501.10289},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2501.10289}, 
}


@article{piecewiseconstant,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2287816},
 abstract = {This paper unites two different fields, survival and contingency table analysis, in a single analytical framework based on the log-linear model. We demonstrate that many currently popular approaches to modeling survival data, including the approaches of Glasser (1967), Cox (1972), Breslow (1972, 1974), and Holford (1976), can be handled by using existing computer packages developed for the log-linear analysis of contingency table data. More important, we demonstrate that the log-linear modeling system used to characterize counted data structures directly characterizes survival data as well. Counted data methodologies for testing and estimation are also applicable here. Much of the theoretical basis for this work has been independently derived by Holford (1980) and Aitkin and Clayton (1980). The emphasis in this paper is not to develop new methodologies, but rather to present new uses and interpretations for already familiar methodologies.},
 author = {Nan Laird and Donald Olivier},
 journal = {Journal of the American Statistical Association},
 number = {374},
 pages = {231--240},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Covariance Analysis of Censored Survival Data Using Log-Linear Analysis Techniques},
 urldate = {2025-08-22},
 volume = {76},
 year = {1981}
}

@article{emulateempareg,
	title = {Type 2 diabetes, sodium-glucose cotransporter-2 inhibitors and cardiovascular outcomes: real world evidence versus a randomised clinical trial},
	volume = {24},
	issn = {1475-2840},
	url = {https://doi.org/10.1186/s12933-025-02924-0},
	doi = {10.1186/s12933-025-02924-0},
	abstract = {Sodium-glucose cotransporter-2 inhibitors ({SGLT}2i) have demonstrated cardiovascular benefits in randomised controlled trials ({RCT}). However, the controlled nature of {RCTs} and the selected trial populations limit their generalizability to real-world practice. Substantial methodological advances now enable robust estimation of absolute risks, risk differences, and continuous on-treatment effects, providing more clinically interpretable measures of {SGLT}2i effectiveness than previously possible with traditional models reliant on hazard ratios.},
	pages = {371},
	number = {1},
	journaltitle = {Cardiovascular Diabetology},
	shortjournal = {Cardiovascular Diabetology},
	author = {Yazdanfard, Puriya Daniel Würtz and Sørensen, Kathrine Kold and Zareini, Bochra and Pedersen-Bjergaard, Ulrik and Ohlendorff, Johan Sebastian and Munch, Anders and Andersen, Mikkel Porsborg and Hasselbalch, Rasmus Bo and Imberg, Henrik and Tasseleus, Viktor and Lind, Marcus and Valabhji, Jonathan and Choudhary, Pratik and Khunti, Kamlesh and Schmid, Stefanie and Lanzinger, Stefanie and Mader, Julia and Gerds, Thomas Alexander and Torp-Pedersen, Christian and {The REDDIE consortium}},
	date = {2025-09-30},
}

@article{sequentialRegressionOhlendorff,
 author = {Johan Sebastian Ohlendorff and Anders Munch and Thomas Alexander Gerds},
 title = {A Novel Sequential Regression Approach for Efficient Continuous-Time Causal Inference},
 year = {2025}
}

@article{shirakawaContinuousTime,
 author = {Toru Shirakawa},
 title = {Deep LTMLE+: Scalable Causal Survival Analysis with Continuous Time, Natural Visits, and Competing Risks},
 year = {2024}
}


@article{rubin_imputation_2005,
	title = {A General Imputation Methodology for Nonparametric Regression with Censored Data},
	url = {https://biostats.bepress.com/ucbbiostat/paper194},
	journaltitle = {U.C. Berkeley Division of Biostatistics Working Paper Series},
	author = {Rubin, Daniel and van der Laan, Mark J.},
	date = {2005-11-14},
	file = {PDF:/home/johan/Zotero/storage/6G2VTZTG/Rubin - A General Imputation Methodology for Nonparametric Regression with Censored Data.pdf:application/pdf;text/html Attachment:/home/johan/Zotero/storage/6QI8L5W3/paper194.html:text/html},
}

@misc{luedtke_sequential_2018,
	title = {Sequential Double Robustness in Right-Censored Longitudinal Models},
	url = {http://arxiv.org/abs/1705.02459},
	doi = {10.48550/arXiv.1705.02459},
	abstract = {Consider estimating the G-formula for the counterfactual mean outcome under a given treatment regime in a longitudinal study. Bang and Robins provided an estimator for this quantity that relies on a sequential regression formulation of this parameter. This approach is doubly robust in that it is consistent if either the outcome regressions or the treatment mechanisms are consistently estimated. We define a stronger notion of double robustness, termed sequential double robustness, for estimators of the longitudinal G-formula. The definition emerges naturally from a more general definition of sequential double robustness for the outcome regression estimators. An outcome regression estimator is sequentially doubly robust ({SDR}) if, at each subsequent time point, either the outcome regression or the treatment mechanism is consistently estimated. This form of robustness is exactly what one would anticipate is attainable by studying the remainder term of a first-order expansion of the G-formula parameter. We show that a particular implementation of an existing procedure is {SDR}. We also introduce a novel {SDR} estimator, whose development involves a novel translation of ideas used in targeted minimum loss-based estimation to the infinite-dimensional setting.},
	number = {{arXiv}:1705.02459},
	publisher = {{arXiv}},
	author = {Luedtke, Alexander R. and Sofrygin, Oleg and Laan, Mark J. van der and Carone, Marco},
	urldate = {2025-10-02},
	date = {2018-05-17},
	eprinttype = {arxiv},
	eprint = {1705.02459 [stat]},
	keywords = {Statistics - Methodology},
	file = {Preprint PDF:/home/johan/Zotero/storage/4P6LUE87/Luedtke et al. - 2018 - Sequential Double Robustness in Right-Censored Longitudinal Models.pdf:application/pdf;Snapshot:/home/johan/Zotero/storage/N6C89F72/1705.html:text/html},
}


@article{hines_demystifying_2022,
	title = {Demystifying Statistical Learning Based on Efficient Influence Functions},
	volume = {76},
	issn = {0003-1305},
	url = {https://doi.org/10.1080/00031305.2021.2021984},
	doi = {10.1080/00031305.2021.2021984},
	abstract = {Evaluation of treatment effects and more general estimands is typically achieved via parametric modeling, which is unsatisfactory since model misspecification is likely. Data-adaptive model building (e.g., statistical/machine learning) is commonly employed to reduce the risk of misspecification. Naïve use of such methods, however, delivers estimators whose bias may shrink too slowly with sample size for inferential methods to perform well, including those based on the bootstrap. Bias arises because standard data-adaptive methods are tuned toward minimal prediction error as opposed to, for example, minimal {MSE} in the estimator. This may cause excess variability that is difficult to acknowledge, due to the complexity of such strategies. Building on results from nonparametric statistics, targeted learning and debiased machine learning overcome these problems by constructing estimators using the estimand’s efficient influence function under the nonparametric model. These increasingly popular methodologies typically assume that the efficient influence function is given, or that the reader is familiar with its derivation. In this article, we focus on derivation of the efficient influence function and explain how it may be used to construct statistical/machine-learning-based estimators. We discuss the requisite conditions for these estimators to perform well and use diverse examples to convey the broad applicability of the theory.},
	pages = {292--304},
	number = {3},
	journaltitle = {The American Statistician},
	author = {Hines, Oliver and Dukes, Oliver and Diaz-Ordaz, Karla and Vansteelandt, Stijn},
	urldate = {2025-10-03},
	date = {2022-07-03},
	note = {Publisher: {ASA} Website \_eprint: https://doi.org/10.1080/00031305.2021.2021984},
	keywords = {Data-adaptive estimation, Double machine learning, Nonparametric methods, Post-selection inference, Targeted learning},
	file = {Full Text:/home/johan/Zotero/storage/SL86R37E/Hines et al. - 2022 - Demystifying Statistical Learning Based on Efficient Influence Functions.pdf:application/pdf},
}


@incollection{kennedy_semiparametric_2024,
	title = {Semiparametric Doubly Robust Targeted Double Machine Learning: A Review},
	shorttitle = {Semiparametric Doubly Robust Targeted Double Machine Learning},
	abstract = {In this review, we cover the basics of efficient nonparametric parameter estimation (also called functional estimation), with a focus on parameters that arise in causal inference problems. We review both efficiency bounds (i.e., what is the best possible performance for estimating a given parameter?) and the analysis of particular estimators (i.e., what is this estimator's error, and does it attain the efficiency bound?) under weak assumptions.},
	booktitle = {Handbook of Statistical Methods for Precision Medicine},
	publisher = {Chapman and Hall/{CRC}},
	author = {Kennedy, Edward H.},
	date = {2024},
	note = {Num Pages: 30},
}