@article{rytgaardContinuoustimeTargetedMinimum2022,
  title = {Continuous-Time Targeted Minimum Loss-Based Estimation of Intervention-Specific Mean Outcomes},
  author = {Rytgaard, Helene C. and Gerds, Thomas A. and van der Laan, Mark J.},
  year = {2022},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {50},
  number = {5},
  pages = {2469--2491},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/21-AOS2114},
  urldate = {2024-01-16},
  abstract = {This paper generalizes the targeted minimum loss-based estimation (TMLE) framework to allow for estimating the effects of time-varying interventions in settings where both interventions, covariates, and outcome can happen at subject-specific time-points on an arbitrarily fine time-scale. TMLE is a general template for constructing asymptotically linear substitution estimators for smooth low-dimensional parameters in infinite-dimensional models. Existing longitudinal TMLE methods are developed for data where observations are made on a discrete time-grid. We consider a continuous-time counting process model where intensity measures track the monitoring of subjects, and focus on a low-dimensional target parameter defined as the intervention-specific mean outcome at the end of follow-up. To construct our TMLE algorithm for the given statistical estimation problem, we derive an expression for the efficient influence curve and represent the target parameter as a functional of intensities and conditional expectations. The high-dimensional nuisance parameters of our model are estimated and updated in an iterative manner according to separate targeting steps for the involved intensities and conditional expectations. The resulting estimator solves the efficient influence curve equation. We state a general efficiency theorem and describe a highly adaptive lasso estimator for nuisance parameters that allows us to establish asymptotic linearity and efficiency of our estimator under minimal conditions on the underlying statistical model.},
  keywords = {62G05,Causal inference,continuous-time interventions,efficient estimation,Semiparametric model,Targeted minimum loss-based estimation (TMLE),time-varying confounding},
  file = {/home/johan/Zotero/storage/M8NHB426/Rytgaard et al. - 2022 - Continuous-time targeted minimum loss-based estima.pdf}
}

@book{last1995marked,
  title={Marked Point Processes on the Real Line: The Dynamical Approach},
  author={Last, G. and Brandt, A.},
  isbn={9780387945477},
  lccn={95018596},
  series={Probability and Its Applications},
  url={https://link.springer.com/book/9780387945477},
  year={1995},
  publisher={Springer}
}

@misc{sunRoleDiscretizationScales2023,
  title = {The Role of Discretization Scales in Causal Inference with Continuous-Time Treatment},
  author = {Sun, Jinghao and Crawford, Forrest W.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.08840},
  eprint = {2306.08840},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.08840},
  urldate = {2024-02-15},
  abstract = {There are well-established methods for identifying the causal effect of a time-varying treatment applied at discrete time points. However, in the real world, many treatments are continuous or have a finer time scale than the one used for measurement or analysis. While researchers have investigated the discrepancies between estimates under varying discretization scales using simulations and empirical data, it is still unclear how the choice of discretization scale affects causal inference. To address this gap, we present a framework to understand how discretization scales impact the properties of causal inferences about the effect of a time-varying treatment. We introduce the concept of "identification bias", which is the difference between the causal estimand for a continuous-time treatment and the purported estimand of a discretized version of the treatment. We show that this bias can persist even with an infinite number of longitudinal treatment-outcome trajectories. We specifically examine the identification problem in a class of linear stochastic continuous-time data-generating processes and demonstrate the identification bias of the g-formula in this context. Our findings indicate that discretization bias can significantly impact empirical analysis, especially when there are limited repeated measurements. Therefore, we recommend that researchers carefully consider the choice of discretization scale and perform sensitivity analysis to address this bias. We also propose a simple and heuristic quantitative measure for sensitivity concerning discretization and suggest that researchers report this measure along with point and interval estimates in their work. By doing so, researchers can better understand and address the potential impact of discretization bias on causal inference.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Methodology,Statistics - Other Statistics},
  file = {/home/johan/Zotero/storage/GCQCSPLE/Sun and Crawford - 2023 - The role of discretization scales in causal infere.pdf;/home/johan/Zotero/storage/2UB5BF7I/2306.html}
}

@book{pearlCausalityModelsReasoning2009,
  title = {Causality: {{Models}}, {{Reasoning}} and {{Inference}}},
  shorttitle = {Causality},
  author = {Pearl, Judea},
  year = {2009},
  month = aug,
  edition = {2nd},
  publisher = {Cambridge University Press},
  address = {USA},
  abstract = {Written by one of the preeminent researchers in the field, this book provides a comprehensive exposition of modern analysis of causation. It shows how causality has grown from a nebulous concept into a mathematical theory with significant applications in the fields of statistics, artificial intelligence, economics, philosophy, cognitive science, and the health and social sciences. Judea Pearl presents and unifies the probabilistic, manipulative, counterfactual, and structural approaches to causation and devises simple mathematical tools for studying the relationships between causal connections and statistical associations. The book will open the way for including causal analysis in the standard curricula of statistics, artificial intelligence, business, epidemiology, social sciences, and economics. Students in these fields will find natural models, simple inferential procedures, and precise mathematical definitions of causal concepts that traditional texts have evaded or made unduly complicated. The first edition of Causality has led to a paradigmatic change in the way that causality is treated in statistics, philosophy, computer science, social science, and economics. Cited in more than 3,000 scientific publications, it continues to liberate scientists from the traditional molds of statistical thinking. In this revised edition, Judea Pearl elucidates thorny issues, answers readers' questions, and offers a panoramic view of recent advances in this field of research. Causality will be of interests to students and professionals in a wide variety of fields. Anyone who wishes to elucidate meaningful relationships from data, predict effects of actions and policies, assess explanations of reported events, or form theories of causal understanding and causal speech will find this book stimulating and invaluable.},
  isbn = {978-0-521-89560-6}
}

@book{andersenStatisticalModelsBased1993,
  title = {Statistical {{Models Based}} on {{Counting Processes}}},
  author = {Andersen, Per Kragh and Borgan, {\O}rnulf and Gill, Richard D. and Keiding, Niels},
  year = {1993},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer US},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-4348-9},
  urldate = {2024-03-12},
  isbn = {978-0-387-94519-4 978-1-4612-4348-9},
  keywords = {censoring,estimator,likelihood,survival analysis},
  file = {/home/johan/Zotero/storage/M2EFUKWS/Andersen et al. - 1993 - Statistical Models Based on Counting Processes.pdf}
}

@misc{shirakawaLongitudinalTargetedMinimum2024,
  title = {Longitudinal {{Targeted Minimum Loss-based Estimation}} with {{Temporal-Difference Heterogeneous Transformer}}},
  author = {Shirakawa, Toru and Li, Yi and Wu, Yulun and Qiu, Sky and Li, Yuxuan and Zhao, Mingduo and Iso, Hiroyasu and {van der Laan}, Mark},
  year = {2024},
  month = apr,
  number = {arXiv:2404.04399},
  eprint = {2404.04399},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-07-04},
  abstract = {We propose Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to estimate the counterfactual mean of outcome under dynamic treatment policies in longitudinal problem settings. Our approach utilizes a transformer architecture with heterogeneous type embedding trained using temporal-difference learning. After obtaining an initial estimate using the transformer, following the targeted minimum loss-based likelihood estimation (TMLE) framework, we statistically corrected for the bias commonly associated with machine learning algorithms. Furthermore, our method also facilitates statistical inference by enabling the provision of 95\% confidence intervals grounded in asymptotic statistical theory. Simulation results demonstrate our method's superior performance over existing approaches, particularly in complex, long time-horizon scenarios. It remains effective in small-sample, short-duration contexts, matching the performance of asymptotically efficient estimators. To demonstrate our method in practice, we applied our method to estimate counterfactual mean outcomes for standard versus intensive blood pressure management strategies in a real-world cardiovascular epidemiology cohort study.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/home/johan/Zotero/storage/A85PMYRL/Shirakawa et al. - 2024 - Longitudinal Targeted Minimum Loss-based Estimatio.pdf}
}

@misc{schulerSelectivelyAdaptiveLasso2022,
  title = {The {{Selectively Adaptive Lasso}}},
  author = {Schuler, Alejandro and {van der Laan}, Mark},
  year = {2022},
  month = may,
  journal = {arXiv.org},
  urldate = {2024-10-03},
  abstract = {Machine learning regression methods allow estimation of functions without unrealistic parametric assumptions. Although they can perform exceptionally in prediction error, most lack theoretical convergence rates necessary for semi-parametric efficient estimation (e.g. TMLE, AIPW) of parameters like average treatment effects. The Highly Adaptive Lasso (HAL) is the only regression method proven to converge quickly enough for a meaningfully large class of functions, independent of the dimensionality of the predictors. Unfortunately, HAL is not computationally scalable. In this paper we build upon the theory of HAL to construct the Selectively Adaptive Lasso (SAL), a new algorithm which retains HAL's dimension-free, nonparametric convergence rate but which also scales computationally to massive datasets. To accomplish this, we prove some general theoretical results pertaining to empirical loss minimization in nested Donsker classes. Our resulting algorithm is a form of gradient tree boosting with an adaptive learning rate, which makes it fast and trivial to implement with off-the-shelf software. Finally, we show that our algorithm retains the performance of standard gradient boosting on a diverse group of real-world datasets. SAL makes semi-parametric efficient estimators practically possible and theoretically justifiable in many big data settings.},
  howpublished = {https://arxiv.org/abs/2205.10697v1},
  langid = {english},
  file = {/home/johan/Zotero/storage/CY39YPCL/Schuler and van der Laan - 2022 - The Selectively Adaptive Lasso.pdf}
}

@misc{liguoriModelingEventsInteractions2023,
  title = {Modeling {{Events}} and {{Interactions}} through {{Temporal Processes}} -- {{A Survey}}},
  author = {Liguori, Angelica and Caroprese, Luciano and Minici, Marco and Veloso, Bruno and Spinnato, Francesco and Nanni, Mirco and Manco, Giuseppe and Gama, Joao},
  year = {2023},
  month = jul,
  number = {arXiv:2303.06067},
  eprint = {2303.06067},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.06067},
  urldate = {2024-05-15},
  abstract = {In real-world scenario, many phenomena produce a collection of events that occur in continuous time. Point Processes provide a natural mathematical framework for modeling these sequences of events. In this survey, we investigate probabilistic models for modeling event sequences through temporal processes. We revise the notion of event modeling and provide the mathematical foundations that characterize the literature on the topic. We define an ontology to categorize the existing approaches in terms of three families: simple, marked, and spatio-temporal point processes. For each family, we systematically review the existing approaches based based on deep learning. Finally, we analyze the scenarios where the proposed techniques can be used for addressing prediction and modeling aspects.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/johan/Zotero/storage/TJN7L7VQ/Liguori et al. - 2023 - Modeling Events and Interactions through Temporal .pdf;/home/johan/Zotero/storage/ZU7AZWY7/2303.html}
}

@inproceedings{weissForestBasedPointProcess2013,
  title = {Forest-{{Based Point Process}} for {{Event Prediction}} from {{Electronic Health Records}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Weiss, Jeremy C. and Page, David},
  editor = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
  year = {2013},
  pages = {547--562},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-40994-3_35},
  abstract = {Accurate prediction of future onset of disease from Electronic Health Records (EHRs) has important clinical and economic implications. In this domain the arrival of data comes at semi-irregular intervals and makes the prediction task challenging. We propose a method called multiplicative-forest point processes (MFPPs) that learns the rate of future events based on an event history. MFPPs join previous theory in multiplicative forest continuous-time Bayesian networks and piecewise-continuous conditional intensity models. We analyze the advantages of using MFPPs over previous methods and show that on synthetic and real EHR forecasting of heart attacks, MFPPs outperform earlier methods and augment off-the-shelf machine learning algorithms.},
  isbn = {978-3-642-40994-3},
  langid = {english},
  file = {/home/johan/Zotero/storage/I4XYNKSV/Weiss and Page - 2013 - Forest-Based Point Process for Event Prediction fr.pdf}
}

@article{rubinEstimatingCausalEffects2005,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/27590541},
 abstract = {Causal effects are defined as comparisons of potential outcomes under different treatments on a common set of units. Observed values of the potential outcomes are revealed by the assignment mechanism—a probabilistic model for the treatment each unit receives as a function of covariates and potential outcomes. Fisher made tremendous contributions to causal inference through his work on the design of randomized experiments, but the potential outcomes perspective applies to other complex experiments and nonrandomized studies as well. As noted by Kempthorne in his 1976 discussion of Savage's Fisher lecture, Fisher never bridged his work on experimental design and his work on parametric modeling, a bridge that appears nearly automatic with an appropriate view of the potential outcomes framework, where the potential outcomes and covariates are given a Bayesian distribution to complete the model specification. Also, this framework crisply separates scientific inference for causal effects and decisions based on such inference, a distinction evident in Fisher's discussion of tests of significance versus tests in an accept/reject framework. But Fisher never used the potential outcomes framework, originally proposed by Neyman in the context of randomized experiments, and as a result he provided generally flawed advice concerning the use of the analysis of covariance to adjust for posttreatment concomitants in randomized trials.},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {469},
 pages = {322--331},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Causal Inference Using Potential Outcomes: Design, Modeling, Decisions},
 urldate = {2024-10-09},
 volume = {100},
 year = {2005}
}

@article{RobinsLongitudinal2001,
author = {Richard D. Gill and James M. Robins},
title = {{Causal Inference for Complex Longitudinal Data: The Continuous Case}},
volume = {29},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {1785 -- 1811},
keywords = {causality, counterfactuals, longitudinal data, observational studies},
year = {2001},
doi = {10.1214/aos/1015345962},
URL = {https://doi.org/10.1214/aos/1015345962}
}

@article{robins1986,
title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
journal = {Mathematical Modelling},
volume = {7},
number = {9},
pages = {1393-1512},
year = {1986},
issn = {0270-0255},
doi = {https://doi.org/10.1016/0270-0255(86)90088-6},
url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
author = {James Robins},
abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.}
}

@article{chernozhukovDoubleMachineLearning2018,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = "{Double/debiased machine learning for treatment and structural parameters}",
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    month = {01},
    abstract = "{We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.}",
    issn = {1368-4221},
    doi = {10.1111/ectj.12097},
    url = {https://doi.org/10.1111/ectj.12097},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}

@article{laanTargetedMaximumLikelihood2006,
  title = {Targeted {{Maximum Likelihood Learning}}},
  author = {{van der Laan}, Mark J. and Rubin, Daniel},
  year = {2006},
  month = dec,
  journal = {The International Journal of Biostatistics},
  volume = {2},
  number = {1},
  publisher = {De Gruyter},
  issn = {1557-4679},
  doi = {10.2202/1557-4679.1043},
  urldate = {2024-01-02},
  abstract = {Suppose one observes a sample of independent and identically distributed observations from a particular data generating distribution. Suppose that one is concerned with estimation of a particular pathwise differentiable Euclidean parameter. A substitution estimator evaluating the parameter of a given likelihood based density estimator is typically too biased and might not even converge at the parametric rate: that is, the density estimator was targeted to be a good estimator of the density and might therefore result in a poor estimator of a particular smooth functional of the density. In this article we propose a one step (and, by iteration, k-th step) targeted maximum likelihood density estimator which involves 1) creating a hardest parametric submodel with parameter epsilon through the given density estimator with score equal to the efficient influence curve of the pathwise differentiable parameter at the density estimator, 2) estimating epsilon with the maximum likelihood estimator, and 3) defining a new density estimator as the corresponding update of the original density estimator. We show that iteration of this algorithm results in a targeted maximum likelihood density estimator which solves the efficient influence curve estimating equation and thereby yields a locally efficient estimator of the parameter of interest, under regularity conditions. In particular, we show that, if the parameter is linear and the model is convex, then the targeted maximum likelihood estimator is often achieved in the first step, and it results in a locally efficient estimator at an arbitrary (e.g., heavily misspecified) starting density.We also show that the targeted maximum likelihood estimators are now in full agreement with the locally efficient estimating function methodology as presented in Robins and Rotnitzky (1992) and van der Laan and Robins (2003), creating, in particular, algebraic equivalence between the double robust locally efficient estimators using the targeted maximum likelihood estimators as an estimate of its nuisance parameters, and targeted maximum likelihood estimators. In addition, it is argued that the targeted MLE has various advantages relative to the current estimating function based approach. We proceed by providing data driven methodologies to select the initial density estimator for the targeted MLE, thereby providing data adaptive targeted maximum likelihood estimation methodology. We illustrate the method with various worked out examples.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {causal effect,cross-validation,efficient influence curve,estimating function,locally efficient estimation,loss function,maximum likelihood estimation,sieve,targeted maximum likelihood estimation,variable importance},
  file = {/home/johan/Zotero/storage/2TAEU6FE/Laan and Rubin - 2006 - Targeted Maximum Likelihood Learning.pdf}
}

@article{laanTargetedMinimumLoss2012,
  title = {Targeted {{Minimum Loss Based Estimation}} of {{Causal Effects}} of {{Multiple Time Point Interventions}}},
  author = {{van der Laan}, Mark J. and Gruber, Susan},
  year = {2012},
  month = may,
  journal = {The International Journal of Biostatistics},
  volume = {8},
  number = {1},
  publisher = {De Gruyter},
  issn = {1557-4679},
  doi = {10.1515/1557-4679.1370},
  urldate = {2024-01-02},
  abstract = {We consider estimation of the effect of a multiple time point intervention on an outcome of interest, where the intervention nodes are subject to time-dependent confounding by intermediate covariates.In previous work van der Laan (2010) and Stitelman and van der Laan (2011a) developed and implemented a closed form targeted maximum likelihood estimator (TMLE) relying on the log-likelihood loss function, and demonstrated important gains relative to inverse probability of treatment weighted estimators and estimating equation based estimators. This TMLE relies on an initial estimator of the entire probability distribution of the longitudinal data structure. To enhance the finite sample performance of the TMLE of the target parameter it is of interest to select the smallest possible relevant part of the data generating distribution, which is estimated and updated by TMLE. Inspired by this goal, we develop a new closed form TMLE of an intervention specific mean outcome based on general longitudinal data structures. The target parameter is represented as an iterative sequence of conditional expectations of the outcome of interest. This collection of conditional means represents the relevant part, which is estimated and updated using the general TMLE algorithm. We also develop this new TMLE for other causal parameters, such as parameters defined by working marginal structural models. The theoretical properties of the TMLE are also practically demonstrated with a small scale simulation study.The proposed TMLE is building upon a previously proposed estimator Bang and Robins (2005) by integrating some of its key and innovative ideas into the TMLE framework.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {Asymptotic linearity of an estimator,causal effect,confounding,efficient influence curve,G-computation formula,influence curve,longitudinal data,loss function,marginal structural working model,nonparametric structural equation model,positivity assumption,randomization assumption,semiparametric statistical model,targeted maximum likelihood estimation,targeted minimum loss based estimation,TMLE,treatment regimen},
  file = {/home/johan/Zotero/storage/HFG2787X/Laan and Gruber - 2012 - Targeted Minimum Loss Based Estimation of Causal E.pdf}
}

@article{discretizationGuerra2020,
author = {Ferreira Guerra, Steve and Schnitzer, Mireille E. and Forget, Amélie and Blais, Lucie},
title = {Impact of discretization of the timeline for longitudinal causal inference methods},
journal = {Statistics in Medicine},
volume = {39},
number = {27},
pages = {4069-4085},
keywords = {coarsening, cross-validation, electronic health data, semiparametric estimation, TMLE},
doi = {https://doi.org/10.1002/sim.8710},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8710},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.8710},
abstract = {In longitudinal settings, causal inference methods usually rely on a discretization of the patient timeline that may not reflect the underlying data generation process. This article investigates the estimation of causal parameters under discretized data. It presents the implicit assumptions practitioners make but do not acknowledge when discretizing data to assess longitudinal causal parameters. We illustrate that differences in point estimates under different discretizations are due to the data coarsening resulting in both a modified definition of the parameter of interest and loss of information about time-dependent confounders. We further investigate several tools to advise analysts in selecting a timeline discretization for use with pooled longitudinal targeted maximum likelihood estimation for the estimation of the parameters of a marginal structural model. We use a simulation study to empirically evaluate bias at different discretizations and assess the use of the cross-validated variance as a measure of data support to select a discretization under a chosen data coarsening mechanism. We then apply our approach to a study on the relative effect of alternative asthma treatments during pregnancy on pregnancy duration. The results of the simulation study illustrate how coarsening changes the target parameter of interest as well as how it may create bias due to a lack of appropriate control for time-dependent confounders. We also observe evidence that the cross-validated variance acts well as a measure of support in the data, by being minimized at finer discretizations as the sample size increases.},
year = {2020}
}

@Inbook{Rose2011,
author="Rose, Sherri and {van der Laan}, Mark J.",
title="Introduction to TMLE",
bookTitle="Targeted Learning: Causal Inference for Observational and Experimental Data",
year="2011",
publisher="Springer New York",
address="New York, NY",
pages="67--82",
abstract="This is the second chapter in our text to deal with estimation.We started by defining the research question. This included our data, model for the probability distribution that generated the data, and the target parameter of the probability distribution of the data. We then presented the estimation of prediction functions using super learning. This leads us to the estimation of causal effects using the TMLE. This chapter introduces TMLE, and a deeper understanding of this methodology is provided in Chap. 5. Note that we use the abbreviation TMLE for targeted maximum likelihood estimation and the targeted maximum likelihood estimator. Later in this text, we discuss targeted minimum loss-based estimation, which can also be abbreviated TMLE.",
isbn="978-1-4419-9782-1",
doi="10.1007/978-1-4419-9782-1_4",
url="https://doi.org/10.1007/978-1-4419-9782-1_4"
}

@book{jacobsen2006point,
  title={Point Process Theory and Applications: Marked Point and Piecewise Deterministic Processes},
  author={Jacobsen, M.},
  isbn={9780817644635},
  lccn={2005934409},
  series={Probability and Its Applications},
  url={https://doi.org/10.1007/0-8176-4463-6},
  year={2006},
  publisher={Birkh{\"a}user Boston}
}

@article{roeysland2024,
    author = {Røysland, Kjetil and C. Ryalen, Pål and Nygård, Mari and Didelez, Vanessa},
    title = {Graphical criteria for the identification of marginal causal effects in continuous-time survival and event-history analyses},
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    pages = {qkae056},
    year = {2024},
    month = {07},
    abstract = {We consider continuous-time survival and event-history settings, where our aim is to graphically represent causal structures allowing us to characterize when a causal parameter is identified from observational data. This causal parameter is formalized as the effect on an outcome event of a (possibly hypothetical) intervention on the intensity of a treatment process. To establish identifiability, we propose novel graphical rules indicating whether the observed information is sufficient to obtain the desired causal effect by suitable reweighting. This requires a different type of graph than in discrete time. We formally define causal semantics for the corresponding dynamic graphs that represent local independence models for multivariate counting processes. Importantly, our work highlights that causal inference from censored data relies on subtle structural assumptions on the censoring process beyond independent censoring; these can be verified graphically. Put together, our results are the first to establish graphical rules for nonparametric causal identifiability in event processes in this generality for the continuous-time case, not relying on particular parametric survival models. We conclude with a data example on Human papillomavirus (HPV) testing for cervical cancer screening, where the assumptions are illustrated graphically and the desired effect is estimated by reweighted cumulative incidence curves.},
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae056},
    url = {https://doi.org/10.1093/jrsssb/qkae056},
    eprint = {https://academic.oup.com/jrsssb/advance-article-pdf/doi/10.1093/jrsssb/qkae056/58499481/qkae056.pdf},
}


@article{van2010collaborative,
  title={Collaborative double robust targeted maximum likelihood estimation},
  author={{van der Laan}, Mark J and Gruber, Susan},
  journal={The international journal of biostatistics},
  volume={6},
  number={1},
  year={2010},
  publisher={De Gruyter}
}

@article{hubbard2016statistical,
  title={Statistical inference for data adaptive target parameters},
  author={Hubbard, Alan E and Kherad-Pajouh, Sara and {van der Laan}, Mark J},
  journal={The international journal of biostatistics},
  volume={12},
  number={1},
  pages={3--19},
  year={2016},
  publisher={De Gruyter}
}


@misc{gill2023causalinferencecomplexlongitudinal,
      title={Causal Inference for Complex Longitudinal Data: The Continuous Time g-Computation Formula}, 
      author={R. D. Gill and J. M. Robins},
      year={2023},
      eprint={math/0409436},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/math/0409436}, 
}

@article{ryalenPotentialOutcomes,
      title = {On the role of martingales in continuous-time causal inference},
      author={Pål Ryalen},
      year={2024},
      publisher={Unpublished},
}

@book{cohen2015stochastic,
  title={Stochastic calculus and applications},
  author={Cohen, Samuel N and Elliott, Robert James},
  volume={2},
  year={2015},
  publisher={Springer}
}